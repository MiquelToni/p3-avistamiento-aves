{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Avistamiento de aves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos (scraping)\n",
    "\n",
    "Primero vamos a obtener los datos. Para ello nos visitaremos las web de avistamientos de aves [shorebirder](https://www.shorebirder.com/), [trevorsbirding](https://www.trevorsbirding.com/) y [dantallmansbirdblog](https://dantallmansbirdblog.blogspot.com/).\n",
    "\n",
    "Durante la visita a la web y haciendo uso del inspector (F12) podemos ver que las descripciones que necesitamos se encuentran en los tag de párrafo (entre *\\<p\\> TEXTO \\</p\\>*). Sabiendo eso vamos a crear funciones de utilidad que se encargarán de descargar el contenido de la web y extraer el texto.\n",
    "\n",
    "Las descargas las realizaremos en `data/raw` mientras que en `data/posts` guardaremos los textos encontrados.\n",
    "\n",
    "** `dantallmansbirdblog` tiene una estructura ligeramente diferente (entre *\\<p\\>\\</p\\> TEXTO \\<p\\>\\</p\\>*), a lo que tendremos que modificar la función `get_texts` (a continuación) para obtener sus textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O utils\n",
    "import os\n",
    "from os.path import exists\n",
    "import re\n",
    "import wget\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "# auxiliar regex\n",
    "remove_parentesis_text = re.compile(r'\\(.*\\)')\n",
    "sentence_endings = re.compile(r'\\.|\\*')\n",
    "\n",
    "# auxiliar functions\n",
    "def maybe_mkdir(path):\n",
    "  try:\n",
    "    if not exists(path):\n",
    "      os.mkdir(path)\n",
    "  except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "def download(url, out_label):\n",
    "  filepath = f\"{data_raw_path}/{out_label}\"\n",
    "  if exists(filepath):\n",
    "    os.remove(filepath)\n",
    "  return wget.download(url, out=filepath)\n",
    "\n",
    "def get_texts(filename):\n",
    "  file = open(filename, 'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "\n",
    "  # get texts\n",
    "  get_p = re.compile(r'<p>((.|\\n)*?)</p>')\n",
    "  texts = get_p.findall(text)\n",
    "\n",
    "  # remove styling and inner tags\n",
    "  remove_tags = re.compile(r'(<.*?>)|\\\\n| +(?= )|\\\\|\\&.+?\\;')\n",
    "  return map(lambda text: re.sub(remove_tags, \"\", str(text[0]).lower()), texts)\n",
    "\n",
    "def write(path, filename, data):\n",
    "  filepath = f\"{path}/{filename}.txt\"\n",
    "  file = open(filepath, \"a\", encoding=\"utf-8\")\n",
    "  for item in data:\n",
    "    file.write(str(item)+\"\\n\")\n",
    "  file.close()\n",
    "  return filepath\n",
    "\n",
    "def save_json(path, filename, dict):\n",
    "  filepath = f\"{path}/{filename}.json\"\n",
    "  # create json object from dictionary\n",
    "  parsed_json = json.dumps(dict)\n",
    "  f = open(filepath,\"w\")\n",
    "  # write json object to file\n",
    "  f.write(parsed_json)\n",
    "  # close file\n",
    "  f.close()\n",
    "  return filepath\n",
    "\n",
    "# constants\n",
    "data_posts_cache = \"../data/cache\" # guardar resultados de queries a sparql\n",
    "data_raw_path = \"../data/raw\" # descargas\n",
    "data_posts_path = \"../data/posts\" # guardar los textos de los post scrapeados\n",
    "data_results_path = \"../data/results\" # guardar los resultados de las diferentes pruebas\n",
    "\n",
    "# build directory structure\n",
    "maybe_mkdir(\"../models\")\n",
    "maybe_mkdir(\"../models/entity_ruler\")\n",
    "maybe_mkdir(\"../data\")\n",
    "maybe_mkdir(\"../owl\")\n",
    "maybe_mkdir(data_raw_path)\n",
    "maybe_mkdir(data_posts_path)\n",
    "maybe_mkdir(data_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación haciendo uso de las funciones anteriores scrapeamos la home de `shorebirder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my late march solo visit to norway is in the books and was about as much fun as i've had in a while. the middle few days of the trip were spent birding around varanger, bookended by more touristy time intromsÃ£Â¸and oslo. at some point in the coming months there will be a full trip report here plus a very detailed cloudbirders submission. in the meantime, here is some proof that i actually went.\\n\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrap shorebirder.com\n",
    "reviews_filename = \"reviews\"\n",
    "shorebirder_filename = \"shorebirder_home.html\"\n",
    "shorebirder_home = download(\"https://www.shorebirder.com/\", shorebirder_filename)\n",
    "posts = get_texts(shorebirder_home)\n",
    "shorebirder_posts_file = write(data_posts_path, reviews_filename, posts)\n",
    "\n",
    "open(shorebirder_posts_file, \"r\").readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para `trevorsbirding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earlier this week i glanced out of my sunroom window to check whether there were any birds at my birdbaths. i currently have three birdbaths just outside the room, one on the ground, one on a pedestal at about 60cm and one hanging from a tree branch at a height of about 1.5 metres. i was delighted to see a small flock of purple-crowned lorikeets having a drink and dipping into the water for a bath. i have just checked my list of species to have visited the birdbaths. this was bird species number 36, in addition to the three reptiles and two mammal species.\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrap trevorsbirding.com\n",
    "trevorsbirding_filename = \"trevorsbirding_home.html\"\n",
    "trevorsbirding_home = download(\"https://www.trevorsbirding.com/\", trevorsbirding_filename)\n",
    "posts = get_texts(trevorsbirding_home)\n",
    "trevorsbirding_posts_file = write(data_posts_path, reviews_filename, posts)\n",
    "\n",
    "open(trevorsbirding_posts_file, \"r\").readlines()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se han obtenido las descripciones de `dantallmansbirdblog` dado que con los dos primeros ya cubrimos el \"mínimo\" frases de posibles avistamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 1: Usar spacy sin modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dependencias para el nlp\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">after a moment or \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", i counted up to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    seven\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " individuals in or near the birdbaths. soon they were joined by several house sparrows and \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " new holland honeyeaters. there was a sudden screeching and a flurry of wings as a collared sparrowhawk swooped in to break up the party. i didnt see if it caught anything for its supper.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the purple-crowned lorikeet is a common bird in the murray bridge district of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    south australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " where i live. on most occasions, however, i hear them go screeching overhead at speed and rarely get good views of them like in todays photos. sometimes i am lucky enough to see them reasonably close when they land to feed on the blossoms of one of the mallee trees on my land. that is when my binoculars help out, as well as the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    83x\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " zoom on my camera.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prueba\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[3:5]:\n",
    "  train = nlp(text)\n",
    "  displacy.render(train, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es capaz de identificar diferentes entidades dentro de las frases, pero no pájaros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 2: Usando sparql query para encontrar aves\n",
    "\n",
    "Idea: Usando el tokenizer de spacy como tokenizer trocear las frases. A partir de los token etiquetados como nombre (`NOUN`) lanzamos una petición a la dbpedia. Ya el resultado de la dbpedia nos dirá si existe y cual es su etiqueta / url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparQL class extension\n",
    "# Prefixes and Class based from https://github.com/ejrav/pydbpedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class SparqlEndpoint(object):\n",
    "\n",
    "    def __init__(self, endpoint, prefixes={}):\n",
    "        self.sparql = SPARQLWrapper(endpoint)\n",
    "        self.prefixes = {\n",
    "            \"dbo\": \"http://dbpedia.org/ontology/\",\n",
    "            \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "            \"dbpedia2\": \"http://dbpedia.org/property/\",\n",
    "            \"dbpedia\": \"http://dbpedia.org/\",\n",
    "            \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"yago\": \"http://dbpedia.org/class/yago/\",\n",
    "            }\n",
    "        self.prefixes.update(prefixes)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "\n",
    "    def query(self, q):\n",
    "        lines = [\"PREFIX %s: <%s>\" % (k, r) for k, r in self.prefixes.items()]\n",
    "        lines.extend(q.split(\"\\n\"))\n",
    "        query = \"\\n\".join(lines)\n",
    "        self.sparql.setQuery(query)\n",
    "        results = self.sparql.query().convert()\n",
    "        return results[\"results\"][\"bindings\"]\n",
    "\n",
    "\n",
    "class DBpediaEndpoint(SparqlEndpoint):\n",
    "    def __init__(self, endpoint, prefixes = {}):\n",
    "        super(DBpediaEndpoint, self).__init__(endpoint, prefixes)\n",
    "\n",
    "s = DBpediaEndpoint(endpoint = \"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bird': {'type': 'uri',\n",
       "   'value': 'http://dbpedia.org/resource/Collared_falconet'},\n",
       "  'name': {'type': 'literal', 'xml:lang': 'en', 'value': 'Collared falconet'},\n",
       "  'comment': {'type': 'literal',\n",
       "   'xml:lang': 'en',\n",
       "   'value': 'The collared falconet (Microhierax caerulescens) is a species of bird of prey in the family Falconidae. It is found in the Indian Subcontinent and Southeast Asia, ranging across Bangladesh, Bhutan, Cambodia, India, Laos, Myanmar, Nepal, Thailand, Malaysia, and Vietnam.Its natural habitat is temperate forest, often on the edges of broadleaf forest. It is 18 cm long. Rapid wingbeats are interspersed with long glides. When perched, it is described as being \"rather shrikelike.\"'}},\n",
       " {'bird': {'type': 'uri',\n",
       "   'value': 'http://dbpedia.org/resource/Peregrine_falcon'},\n",
       "  'name': {'type': 'literal', 'xml:lang': 'en', 'value': 'Peregrine falcon'},\n",
       "  'comment': {'type': 'literal',\n",
       "   'xml:lang': 'en',\n",
       "   'value': 'The peregrine falcon (Falco peregrinus), also known as the peregrine, and historically as the duck hawk in North America, is a cosmopolitan bird of prey (raptor) in the family Falconidae. A large, crow-sized falcon, it has a blue-grey back, barred white underparts, and a black head. The peregrine is renowned for its speed, reaching over 320 km/h (200 mph) during its characteristic hunting stoop (high-speed dive), making it the fastest bird in the world, as well as the fastest member of the animal kingdom. According to a National Geographic TV program, the highest measured speed of a peregrine falcon is 389 km/h (242 mph). As is typical for bird-eating raptors, peregrine falcons are sexually dimorphic, with females being considerably larger than males. The peregrine\\'s breeding range includes land regions from the Arctic tundra to the tropics. It can be found nearly everywhere on Earth, except extreme polar regions, very high mountains, and most tropical rainforests; the only major ice-free landmass from which it is entirely absent is New Zealand. This makes it the world\\'s most widespread raptor, and one of the most widely found bird species. In fact, the only land-based bird species found over a larger geographic area is not always naturally occurring, but one widely introduced by humans, the rock pigeon, which in turn now supports many peregrine populations as a prey species. The peregrine is a highly successful example of urban wildlife in much of its range, taking advantage of tall buildings as nest sites and an abundance of prey such as pigeons and ducks. Both the English and scientific names of this species mean \"wandering falcon,\" referring to the migratory habits of many northern populations. Experts recognize 17 to 19 subspecies, which vary in appearance and range; disagreement exists over whether the distinctive Barbary falcon is represented by two subspecies of Falco peregrinus, or is a separate species, F. pelegrinoides. The two species\\' divergence is relatively recent, during the time of the last ice age, therefore the genetic differential between them (and also the difference in their appearance) is relatively tiny. They are only about 0.6–0.8% genetically differentiated. Although its diet consists almost exclusively of medium-sized birds, the peregrine will sometimes hunt small mammals, small reptiles, or even insects. Reaching sexual maturity at one year, it mates for life and nests in a scrape, normally on cliff edges or, in recent times, on tall human-made structures. The peregrine falcon became an endangered species in many areas because of the widespread use of certain pesticides, especially DDT. Since the ban on DDT from the early 1970s, populations have recovered, supported by large-scale protection of nesting places and releases to the wild. The peregrine falcon is a well-respected falconry bird due to its strong hunting ability, high trainability, versatility, and availability via captive breeding. It is effective on most game bird species, from small to large. It has also been used as a religious, royal, or national symbol across multiple eras and areas of human civilization.'}},\n",
       " {'bird': {'type': 'uri',\n",
       "   'value': \"http://dbpedia.org/resource/Eleonora's_falcon\"},\n",
       "  'name': {'type': 'literal', 'xml:lang': 'en', 'value': \"Eleonora's falcon\"},\n",
       "  'comment': {'type': 'literal',\n",
       "   'xml:lang': 'en',\n",
       "   'value': \"Eleonora's falcon (Falco eleonorae) is a medium-sized falcon. It belongs to the hobby group, a rather close-knit number of similar falcons often considered a subgenus Hypotriorchis. The sooty falcon is sometimes considered its closest relative, but while they certainly belong to the same lineage, they do not seem to be close sister species. The English name and the species name eleonorae commemorate Eleanor of Arborea, Queen or Lady-Judge (Juighissa) and national heroine of Sardinia, who in 1392, under the jurisdiction conferred by the Carta de Logu, became the first ruler in history to grant protection to hawk and falcon nests against illegal hunters. The genus name falco is from Late Latin falx, falcis, a sickle, referring to the claws of the bird.\"}},\n",
       " {'bird': {'type': 'uri',\n",
       "   'value': 'http://dbpedia.org/resource/Red-footed_falcon'},\n",
       "  'name': {'type': 'literal', 'xml:lang': 'en', 'value': 'Red-footed falcon'},\n",
       "  'comment': {'type': 'literal',\n",
       "   'xml:lang': 'en',\n",
       "   'value': \"The red-footed falcon (Falco vespertinus), formerly the western red-footed falcon, is a bird of prey. It belongs to the family Falconidae, the falcons. This bird is found in eastern Europe and Asia although its numbers are dwindling rapidly due to habitat loss and hunting. It is migratory, wintering in Africa. It is a regular wanderer to western Europe, and in August 2004 a red-footed falcon was found in North America for the first time on the island of Martha's Vineyard, Massachusetts.\"}},\n",
       " {'bird': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Altai_falcon'},\n",
       "  'name': {'type': 'literal', 'xml:lang': 'en', 'value': 'Altai falcon'},\n",
       "  'comment': {'type': 'literal',\n",
       "   'xml:lang': 'en',\n",
       "   'value': 'The Altai falcon (Falco cherrug altaicus?) is a large falcon of questionable taxonomic position. It is often considered to be a subspecies of the saker falcon (Falco cherrug). It used to have a high reputation among Central Asian falconers. It is uncertain whether the bird is a saker subspecies or a hybrid.'}}]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para buscar una ave dado su nombre\n",
    "def search_bird_dbpedia(token):\n",
    "  return s.query('''\n",
    "    SELECT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "\n",
    "      filter (CONTAINS(LCASE(STR(?name)), \"{token}\")) .\n",
    "    }\n",
    "    limit 5\n",
    "  '''.replace(\"{token}\", token))\n",
    "\n",
    "search_bird_dbpedia(\"falcon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple\n",
      "lorikeet\n",
      "bird\n",
      "blossoms\n",
      "mallee\n",
      "trees\n",
      "land\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "maybe_matches = {}\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in doc.noun_chunks:\n",
    "    for token in chunk:\n",
    "      if token.pos_ == 'NOUN':\n",
    "        results = search_bird_dbpedia(token.lemma_)\n",
    "        if len(results) > 0:\n",
    "          maybe_matches[token] = results\n",
    "          print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, funciona? Lo que es muy lento y estamos machacando la dbpedia a queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 3: Cachear / indexar la dbpedia\n",
    "\n",
    "Del intento anterior vamos a coger los resultados de todos los pájaros y lo convertiremos en un diccionario para que nos sea más fácil buscar y solo haremos n queries a la dbpedia. Por supuesto, esta estrategia es solo factible si el conjunto es finito. Como es nuestro caso, va haber n especies de pájaros, pero no va a estar creciendo dia a dia.\n",
    "\n",
    "### Estrategia\n",
    "- Obtener lista de todos los nombres de pájaros.\n",
    "- Con spacy analizaremos la entrada del avistamiento y obtenemos los `noun chunk` y `ents`.\n",
    "  - Para obtener los chunk necesitamos las pipelines de `tok2vec`, `tagger`, `parser` y `attribute_ruler`.\n",
    "  - Para obtener las entidades necesitamos `ner`.\n",
    "- Con cada `chunk` usando fuzzy-search en la lista de nombres de pájaros para encontrar aquellos chunk que parezcan nombres de pájaros.\n",
    "- Con cada `ent` nos aportará datos sobre el contexto: ubicación, fechas u otros. Este ent se asociará al pájaro si el ent pertenece al chunk del pájaro y en su defecto al avistamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos obtenido los nombres de 10369 pájaros\n",
      "Actenoides\n",
      "African goshawk\n",
      "African pitta\n",
      "African red-eyed bulbul\n",
      "Alcedo\n"
     ]
    }
   ],
   "source": [
    "# Obtener todos los pájaros con nombre, url y descripción\n",
    "def get_all_birds_sparql(batch_size):\n",
    "  found_all = False\n",
    "  limit = batch_size\n",
    "  offset = 0\n",
    "  query = \"\"\"\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "      \n",
    "    }\n",
    "    limit {limit_value}\n",
    "    offset {offset_value}\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  while not found_all:\n",
    "    loop_result = s.query(query.replace(\"{limit_value}\", str(limit)).replace(\"{offset_value}\", str(offset)))\n",
    "    if len(loop_result) < limit:\n",
    "      found_all = True\n",
    "    else:\n",
    "      offset = limit\n",
    "      limit += batch_size\n",
    "    result += loop_result\n",
    "  return result\n",
    "\n",
    "birds_sparql = get_all_birds_sparql(5000)\n",
    "\n",
    "write(\"../data\", \"birds\", birds_sparql)\n",
    "\n",
    "print(f\"Hemos obtenido los nombres de {len(birds_sparql)} pájaros\")\n",
    "for d in birds_sparql[0:5]:\n",
    "  print(d['name']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los datos en crudo a un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparql a diccionario\n",
    "remove_parentesis_text = re.compile(r'\\(|\\)')\n",
    "\n",
    "birds = {}\n",
    "for bird in birds_sparql:\n",
    "  bird_name_lower = bird[\"name\"][\"value\"].lower()\n",
    "  key = re.sub(remove_parentesis_text, \"\", str(bird_name_lower))\n",
    "  if key in birds.keys():\n",
    "    print(f\"bird {key} is duplicated.\", birds[key][\"name\"], bird[\"name\"][\"value\"])\n",
    "  birds[key] = {\n",
    "    \"name\": bird[\"name\"][\"value\"],\n",
    "    \"url\": bird[\"bird\"][\"value\"],\n",
    "    \"description\": bird[\"comment\"][\"value\"],\n",
    "  }\n",
    "bird_keys = birds.keys() # buscaremos por las key\n",
    "assert len(birds_sparql) == len(bird_keys) # aseguramos que no hayamos perdido ninguna key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Goliath (Mangalia)',\n",
       " 'url': 'http://dbpedia.org/resource/Goliath_(Mangalia)',\n",
       " 'description': 'Goliath is the name of a crane that is currently located at the Mangalia shipyard in Mangalia, Romania. Formerly, it was part of the Fore River Shipyard in Quincy, Massachusetts.'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds[\"goliath mangalia\"] # bueno... la dbpedia tampoco es perfecta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos el `nlp`. Usamos el preset large (`en_core_web_lg`) porque nos proporciona más precisión en el reconocimiento de entidades. Este reconocimiento de entidades lo usaremos para registrar elementos del contexto y completar los individuos de nuestra ontología"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5db9d2511eeb483e8e56620ef312868d-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Black-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">billed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">flycatcher</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5db9d2511eeb483e8e56620ef312868d-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5db9d2511eeb483e8e56620ef312868d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5db9d2511eeb483e8e56620ef312868d-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5db9d2511eeb483e8e56620ef312868d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preparar nlp\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['lemmatizer'])\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el `tokenizer` de spacy nos separa las palabras compuestas con guion.\n",
    "\n",
    "Para solucionarlo vamos a modificar el tokenizer para que no separe las palabras con guion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer que el tokenizer no separe palabras con guion\n",
    "# https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2acbfbbf4fc14e8db5f412bf363fa9bc-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Black-billed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">flycatcher</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2acbfbbf4fc14e8db5f412bf363fa9bc-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2acbfbbf4fc14e8db5f412bf363fa9bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genial! A demás conseguimos que `Black-billed` se detecte como adjetivo y no como adjetivo + verbo.\n",
    "\n",
    "A continuación, creamos las funciones necesarias para detectar los pájaros y una demo/test del funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'purple-crowned lorikeet' in 'purple-crowned lorikeets' with score of 98\n",
      "Found 'banded honeyeater' in 'new honeyeaters' with score of 81\n",
      "Found 'collared sparrowhawk' in 'collared sparrowhawk' with score of 100\n",
      "Found 'purple-crowned lorikeet' in 'purple-crowned lorikeet' with score of 100\n",
      "Found 'common iora' in 'common bird' with score of 82\n",
      "['purple-crowned lorikeet', 'banded honeyeater', 'collared sparrowhawk', 'purple-crowned lorikeet', 'common iora']\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de pájaros por fuzzysearch\n",
    "from fuzzywuzzy import fuzz\n",
    "def search_bird_dict(chunk):\n",
    "  best_match = (False, None, 0)\n",
    "  for key in bird_keys:\n",
    "    score = fuzz.ratio(key, chunk)\n",
    "    if score > best_match[-1]:\n",
    "      # print(f\"\\tchunk: '{chunk}' compare '{key}' score: '{score}'\")\n",
    "      best_match = (score > 80, key, score)\n",
    "  return best_match\n",
    "\n",
    "def get_nouns(chunk):\n",
    "  only_nouns = []\n",
    "  for token in chunk:\n",
    "    if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
    "      only_nouns.append(token.lower_)\n",
    "\n",
    "  if len(only_nouns) > 0:\n",
    "    return \" \".join(only_nouns)\n",
    "  return None\n",
    "\n",
    "maybe_matches = []\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[0:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "  # for chunk in doc.noun_chunks: # aplicar sugerencia a ver si es más rápido así -- 1.0s +/- 0.1s más lento que el anterior -- no y perdemos precisión\n",
    "  #   chunk = str(chunk)\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "        print(f\"Found '{bird_key}' in '{chunk}' with score of {score}\")\n",
    "          \n",
    "# Test result\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el chunk siguiente, definimos como queremos que sea nuestra ontología.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos la ontología\n",
    "# pip install Cython==0.29.28 owlready2==0.37\n",
    "# pip install slugify\n",
    "from owlready2 import *\n",
    "from slugify import slugify\n",
    "import uuid\n",
    "\n",
    "# Creamos una ontología\n",
    "onto = get_ontology(\"http://avistamiento-aves-3.org/onto.owl\")\n",
    "onto.destroy()\n",
    "onto = get_ontology(\"http://avistamiento-aves-3.org/onto.owl\")\n",
    "\n",
    "with onto:\n",
    "  class label(DataProperty):\n",
    "    range = [str]\n",
    "  class match_text(DataProperty):\n",
    "    range = [str]\n",
    "  class match_score(DataProperty):\n",
    "    range = [float]\n",
    "  class dbpedia(DataProperty):\n",
    "    range = [str]\n",
    "  class date(DataProperty):\n",
    "    range = [str]\n",
    "  class position(DataProperty):\n",
    "    range = [str]\n",
    "  class generic_context(DataProperty):\n",
    "    range = [str]\n",
    "\n",
    "  class Bird(Thing):\n",
    "    pass\n",
    "  class Sighting(Thing):\n",
    "    pass\n",
    "\n",
    "  class has_been_seen(ObjectProperty):\n",
    "    domain = [Bird]\n",
    "    range = [Sighting]\n",
    "  class birds_found(ObjectProperty):\n",
    "    domain = [Sighting]\n",
    "    range = [Bird]\n",
    "    inverse_property = has_been_seen\n",
    "\n",
    "# Owl functions\n",
    "def create_sighting(doc):\n",
    "  sighting = Sighting(f\"sighting-{uuid.uuid4()}\")\n",
    "  sighting.text = doc.text\n",
    "  return sighting\n",
    "\n",
    "def register_bird(sighting, bird_key, chunk_nouns, score):\n",
    "  slugify_name = slugify(bird_key, separator=\"_\")\n",
    "  bird_onto = types.new_class(slugify_name, (Bird,))\n",
    "  bird_dict = birds[bird_key]\n",
    "  bird_individual = bird_onto(f\"{slugify_name}-{uuid.uuid4()}\")\n",
    "  bird_individual.label.append(bird_dict[\"name\"])\n",
    "  bird_individual.match_text.append(chunk_nouns)\n",
    "  bird_individual.match_score.append(score)\n",
    "  bird_individual.dbpedia.append(bird_dict[\"url\"])\n",
    "  sighting.birds_found.append(bird_individual)\n",
    "  return bird_individual\n",
    "\n",
    "def add_context(individual, name, type):\n",
    "  if type == \"DATE\":\n",
    "    individual.date.append(name)\n",
    "  if type == \"NORP\" or type == \"GPE\":\n",
    "    individual.position.append(name)\n",
    "  else:\n",
    "    individual.generic_context.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:14<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 20 pájaros de 17 diferentes tipos\n",
      "../data/posts/shorebirder_results_log_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Found 'purple-crowned lorikeet' in 'purple-crowned lorikeets' with score of 98\\n\",\n",
       " \"Found 'banded honeyeater' in 'new honeyeaters' with score of 81\\n\"]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "trace = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  sighting = create_sighting(doc)\n",
    "\n",
    "  for chunk in doc.noun_chunks:\n",
    "    chunk_nouns = get_nouns(chunk)\n",
    "    if chunk_nouns != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk_nouns)\n",
    "      if found:\n",
    "        bird_individual = register_bird(sighting, bird_key, chunk_nouns, score)\n",
    "        trace.append(f\"Found '{bird_key}' in '{chunk_nouns}' with score of {score}\")\n",
    "        for ent in chunk.ents:\n",
    "          add_context(bird_individual, str(ent), ent.label_)\n",
    "      else:\n",
    "        for ent in chunk.ents:\n",
    "          add_context(sighting, str(ent), ent.label_)\n",
    "\n",
    "onto.save(file=\"../owl/avistamiento-aves-3.xml\", format = \"rdfxml\")\n",
    "log_result = write(data_results_path, \"shorebirder_results_log_3\", trace)\n",
    "print(f\"Hemos encontrado {len(trace)} pájaros de {len(list(Bird.subclasses()))} diferentes tipos\")\n",
    "print(log_result)\n",
    "open(log_result, \"r\", encoding=\"utf-8\").readlines()[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera versión funcional sin machacar a la dbpedia. Pero sigue siendo muy lento. Vamos a seguir probando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nueva pipeline [entity_ruler](https://spacy.io/api/entityruler)\n",
    "\n",
    "En esta aproximación vamos a añadir una pipe más al nlp `en_core_web_lg` pre-entrenado de spacy. Para ello necesitamos hacer una lista de todos los patterns que queramos poner. Es decir, debemos introducir los nombres de los pájaros que queremos que se detecten como patterns y añadir la nueva pipe al nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando entity_ruler https://spacy.io/usage/rule-based-matching#entityruler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "tag = \"BIRD\"\n",
    "\n",
    "# init blank nlp\n",
    "nlp = English()\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# Añadir los nombres de pájaros\n",
    "patterns = []\n",
    "for key in bird_keys:\n",
    "  bird = birds[key]\n",
    "  doc = nlp(bird[\"name\"])\n",
    "  pattern = []\n",
    "  for token in doc:\n",
    "    pattern.append({\n",
    "      \"LOWER\": token.lower_\n",
    "    })\n",
    "\n",
    "  patterns.append({\n",
    "    \"label\": tag,\n",
    "    \"pattern\": pattern,\n",
    "    \"id\": key\n",
    "  })\n",
    "\n",
    "# add entity_ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "with nlp.select_pipes(enable=\"tagger\"):\n",
    "  ruler.add_patterns(patterns)\n",
    "\n",
    "nlp.to_disk(\"../models/entity_ruler/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">that feeding \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gull\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BIRD</span>\n",
       "</mark>\n",
       " flock continued to produce by sucking in passers by. at \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " point a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bonaparte's gull\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BIRD</span>\n",
       "</mark>\n",
       " got in on the action, and a flock of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    21\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " common terns appeared from the east and eventually settled into that flock.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"that feeding gull flock continued to produce by sucking in passers by. at one point a Bonaparte's gull got in on the action, and a flock of 21 common terns appeared from the east and eventually settled into that flock.\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clonamos el schema de la ontología del apartado anterior. A esta, tenemos que modificar la función `register_bird` porque en este caso no tenemos `name_chunks` ni un `score` de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos la ontología\n",
    "from owlready2 import *\n",
    "from slugify import slugify\n",
    "import uuid\n",
    "\n",
    "# Creamos una ontología\n",
    "onto = get_ontology(\"http://avistamiento-aves-4.org/onto.owl\")\n",
    "onto.destroy()\n",
    "onto = get_ontology(\"http://avistamiento-aves-4.org/onto.owl\")\n",
    "\n",
    "with onto:\n",
    "  class label(DataProperty):\n",
    "    range = [str]\n",
    "  class match_text(DataProperty):\n",
    "    range = [str]\n",
    "  class match_score(DataProperty):\n",
    "    range = [float]\n",
    "  class dbpedia(DataProperty):\n",
    "    range = [str]\n",
    "  class date(DataProperty):\n",
    "    range = [str]\n",
    "  class position(DataProperty):\n",
    "    range = [str]\n",
    "  class generic_context(DataProperty):\n",
    "    range = [str]\n",
    "\n",
    "  class Bird(Thing):\n",
    "    pass\n",
    "  class Sighting(Thing):\n",
    "    pass\n",
    "\n",
    "  class has_been_seen(ObjectProperty):\n",
    "    domain = [Bird]\n",
    "    range = [Sighting]\n",
    "  class birds_found(ObjectProperty):\n",
    "    domain = [Sighting]\n",
    "    range = [Bird]\n",
    "    inverse_property = has_been_seen\n",
    "\n",
    "# Owl functions\n",
    "def create_sighting(doc):\n",
    "  sighting = Sighting(f\"sighting-{uuid.uuid4()}\")\n",
    "  sighting.text = doc.text\n",
    "  return sighting\n",
    "\n",
    "def register_bird(sighting, bird_key):\n",
    "  slugify_name = slugify(bird_key, separator=\"_\")\n",
    "  bird_onto = types.new_class(slugify_name, (Bird,))\n",
    "  bird_dict = birds[bird_key]\n",
    "  bird_individual = bird_onto(f\"{slugify_name}-{uuid.uuid4()}\")\n",
    "  bird_individual.label.append(bird_dict[\"name\"])\n",
    "  bird_individual.dbpedia.append(bird_dict[\"url\"])\n",
    "  sighting.birds_found.append(bird_individual)\n",
    "  return bird_individual\n",
    "\n",
    "def add_context(individual, name, type):\n",
    "  if type == \"DATE\":\n",
    "    individual.date.append(name)\n",
    "  if type == \"NORP\" or type == \"GPE\":\n",
    "    individual.position.append(name)\n",
    "  else:\n",
    "    individual.generic_context.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 76.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 9 pájaros de 9 diferentes tipos\n",
      "../data/posts/shorebirder_results_log_4.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Found 'collared sparrowhawk'\\n\",\n",
       " \"Found 'purple-crowned lorikeet'\\n\",\n",
       " \"Found 'rainbow lorikeet'\\n\",\n",
       " \"Found 'musk lorikeet'\\n\",\n",
       " \"Found 'rosy-faced lovebird'\\n\"]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "nlp = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "trace = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  sighting = create_sighting(doc)\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == tag and ent.ent_id_ != \"\":\n",
    "      bird_individual = register_bird(sighting, ent.ent_id_)\n",
    "      trace.append(f\"Found '{ent.ent_id_}'\")\n",
    "    else:\n",
    "      add_context(sighting, str(ent), ent.label_)\n",
    "\n",
    "onto.save(file=\"../owl/avistamiento-aves-4.xml\", format = \"rdfxml\")\n",
    "log_result = write(data_posts_path, \"shorebirder_results_log_4\", trace)\n",
    "print(f\"Hemos encontrado {len(trace)} pájaros de {len(list(Bird.subclasses()))} diferentes tipos\")\n",
    "print(log_result)\n",
    "open(log_result, \"r\", encoding=\"utf-8\").readlines()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoramos mucho la velocidad por una cantidad aceptable de reconocimiento de aves. Pero no nos detecta todas las que conseguimos detectar en el `intento 3`. Esto es porque la capa de entity_ruler no es parte del modelo sino pattern matching. por ello no es capaz de encontrar las formas en plural de las aves que el intento 3 si encuentra.\n",
    "Al tampoco usar `ner` perdemos la capacidad de detectar elementos del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenar ner Spacy\n",
    "\n",
    "En este apartado vamos a centrarnos en mejorar la detección de spacy entrenando un modelo de clasificación. Las frases de entreno las cogeremos de las descripciones de la dbpedia. Los datos de entreno deberán estar compuestos de una frase más las posiciones de los nombres de pájaros. Para reconocer los pájaros que aparecen en las descripciones usaremos el `entity ruler` del apartado anterior. Al usar el entity ruler para esta tarea vamos a suponer que en la dbpedia no hay faltas de ortografía en las descripciones para que los pájaros sean identificables, y su nombre aparezca al menos una vez.\n",
    "\n",
    "La [pipeline](https://spacy.io/usage/spacy-101#pipelines) de spacy que se encarga de etiquetar las entities se llama [Entity Recognizer](https://spacy.io/api/entityrecognizer) o `ner`. Según la documentación de spacy sobre como [entrenar](https://spacy.io/usage/training) un modelo, primero necesitamos las frases de entreno correctamente etiquetadas, vectores por palabra a etiquetar y un fichero de configuración con las pipelines a entrenar.\n",
    "\n",
    "La generación de frases de entreno usaremos el 100% de frases para train y 20% para test (subconjunto de train). He decidido \"correr el riesgo\" de over-fitting del modelo a la de no registrar pájaros al modelo. He tomado esta decisión dada las escasas frases/pájaro que podemos obtener de la dbpedia.\n",
    "\n",
    "Para que ner pueda reconocer y comparar las aves vamos a tener que generar [vectores](https://spacy.io/usage/linguistic-features#vectors-similarity). Sabemos que el nlp de spacy no genera vectores para palabras fuera del modelo, o en el caso de `en_core_web_sm` directamente no tiene. Para ello, la generación de vectores se va a realizar mediante la librería [gensim](https://radimrehurek.com/gensim/) y la técnica [Word2Vec](https://es.wikipedia.org/wiki/Word2vec) como sugieren en la documentación de spacy.\n",
    "\n",
    "El fichero de configuración guardado en `src/birds_config.cfg` se ha generado usando la [herramienta web](https://spacy.io/usage/training#quickstart) que proporciona spacy seleccionando `ner` + `cpu` + `efficiency`. Ner, porque es la pipeline que queremos entrenar. Cpu, porque he tenido problemas para compilar pytorch para usar la gpu y es una opción más compatible. Efficiency porque nosotros le proporcionaremos los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\Anaconda3\\lib\\site-packages\\spacy\\language.py:710: UserWarning: [W113] Sourced component 'entity_ruler' may not work as expected: source vectors are not identical to current pipeline vectors.\n",
      "  warnings.warn(Warnings.W113.format(name=source_name))\n",
      "  0%|          | 0/10369 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\text mining\\p3-avistamiento-aves\\src\\p3-avistamiento-aves.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m ent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39ments:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=42'>43</a>\u001b[0m   \u001b[39mif\u001b[39;00m ent\u001b[39m.\u001b[39mlabel_ \u001b[39m==\u001b[39m tag \u001b[39mand\u001b[39;00m ent\u001b[39m.\u001b[39ment_id_ \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=43'>44</a>\u001b[0m     vector \u001b[39m=\u001b[39m get_vector(ent)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=44'>45</a>\u001b[0m     positions\u001b[39m.\u001b[39mappend((ent\u001b[39m.\u001b[39mstart_char, ent\u001b[39m.\u001b[39mend_char, ent\u001b[39m.\u001b[39ment_id_, vector))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(positions) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32md:\\Projects\\text mining\\p3-avistamiento-aves\\src\\p3-avistamiento-aves.ipynb Cell 42'\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(ent)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=18'>19</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m ent\u001b[39m.\u001b[39mvector\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=19'>20</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/text%20mining/p3-avistamiento-aves/src/p3-avistamiento-aves.ipynb#ch0000037?line=20'>21</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mWord2Vec(ent, min_count \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,size \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m, window \u001b[39m=\u001b[39;49m \u001b[39m6\u001b[39;49m, sg\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "# Transformar las descripciones en datos de entreno y test para spacy\n",
    "import gensim\n",
    "import random\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# init the \"base nlp\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# add custom tokenizer\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "# add entity_ruler \n",
    "entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.add_pipe(\"entity_ruler\", source=entity_ruler, before=\"ner\")\n",
    "\n",
    "def get_vector(ent):\n",
    "  if ent.has_vector:\n",
    "    return ent.vector\n",
    "  else:\n",
    "    return gensim.models.Word2Vec(ent, min_count = 1,size = 200, window = 6, sg=0)\n",
    "\n",
    "# build training sentences\n",
    "training_data = [\n",
    "  # (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")], tag/label, vector), # example\n",
    "]\n",
    "for key in tqdm.tqdm(bird_keys):\n",
    "  bird = birds[key]\n",
    "  bird_name = bird[\"name\"]\n",
    "\n",
    "  if len(bird_name) == 0:\n",
    "    continue\n",
    "  \n",
    "  bird_name = re.sub(remove_parentesis_text, \"\", str(bird_name))\n",
    "  description = bird[\"description\"]\n",
    "\n",
    "  for train_sentence in re.split(sentence_endings, description):\n",
    "    # usamos el reconocimiento de aves anterior para encontrar las aves\n",
    "    # suponemos que la dbpedia tiene las aves bien identificadas\n",
    "    doc = nlp(train_sentence)\n",
    "    positions = []\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_ == tag and ent.ent_id_ != \"\":\n",
    "        vector = get_vector(ent)\n",
    "        positions.append((ent.start_char, ent.end_char, ent.ent_id_, vector))\n",
    "\n",
    "    if len(positions) > 0:\n",
    "      training_data.append(\n",
    "        (train_sentence, positions)\n",
    "      )\n",
    "\n",
    "def outer_join(lst1, lst2):\n",
    "  lst2_names = [name for name, annotations in lst2]\n",
    "  lst3 = [(name, annotations) for name, annotations in lst1 if not name in lst2_names]\n",
    "  return lst3\n",
    "\n",
    "test_data = random.sample(training_data, k=round(len(training_data)*0.2))\n",
    "\n",
    "# si quitamos las frases de test del modelo de entreno luego no vamos a poder detectar esas aves... no es ideal pero tampoco podemos asegurar que tengamos frases para todos los pájaros\n",
    "training_data = outer_join(training_data, test_data) \n",
    "\n",
    "for text, annotations in training_data[0:5]:\n",
    "  print(text, [(a, b, c) for a, b, c, d in annotations])\n",
    "  print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 185.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build train set\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = English()\n",
    "# no separar por \"-\"\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "# añadir entity_ruler del apartado anterior\n",
    "entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.add_pipe(\"entity_ruler\", source=entity_ruler)\n",
    "\n",
    "def build_db(nlp, data):\n",
    "  skips = 0\n",
    "  # the DocBin will store the example documents\n",
    "  db = DocBin()\n",
    "  for text, annotations in tqdm.tqdm(data):\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label, vector in annotations:\n",
    "      span = doc.char_span(start, end, label=label, vector=vector)\n",
    "      if span is None:\n",
    "        skips += 1\n",
    "      else:\n",
    "        ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    db.add(doc)\n",
    "  return (db, skips)\n",
    "\n",
    "(db, skips) = build_db(nlp, training_data)\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 136.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build test set\n",
    "(db, skips) = build_db(nlp, test_data)\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model https://spacy.io/usage/training\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# train the model\n",
    "# !python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "# !python -m spacy train config.cfg --output ../models/custom_ner --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy\n",
    "\n",
    "## powershell: (launch from console to use all cpu cores)\n",
    "# $env:KMP_DUPLICATE_LIB_OK = \"True\"\n",
    "# python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "# python -m spacy train config.cfg --output ../models/custom_ner --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    golden-fronteds fulvetta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">grey honeyeater</span>\n",
       "</mark>\n",
       " (Schoeniparus variegaticeps), also known as the gold-fronted fulvetta, is a species of bird in the family Pellorneidae</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    African goshawk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">african goshawk</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Accipiter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">accipiter</span>\n",
       "</mark>\n",
       " tachiro) is a species of African bird of prey in the genus \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Accipiter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">accipiter</span>\n",
       "</mark>\n",
       " which is the type genus of the family Accipitridae.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The type species is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hombron's kingfisher\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">hombron's kingfisher</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Actenoides\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">actenoides</span>\n",
       "</mark>\n",
       " hombroni)</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# juntamos el entreno previo con las patterns\n",
    "nlp = spacy.load(\"../models/custom_ner/model-best\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "# nlp.add_pipe(\"entity_ruler\", source=entity_ruler, before=\"ner\")\n",
    "\n",
    "# ner = spacy.load(\"en_core_web_sm\") # lg tiene vectores propios por lo que no podemos mezclar los vectores generados en nuestro modelo\n",
    "# nlp.add_pipe(\"ner\", name=\"base_ner\", source=ner, after=\"ner\")\n",
    "\n",
    "# comprobamos las pipes\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# try\n",
    "doc = nlp(\"The golden-fronteds fulvetta (Schoeniparus variegaticeps), also known as the gold-fronted fulvetta, is a species of bird in the family Pellorneidae\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "\n",
    "doc = nlp(\"The African goshawk (Accipiter tachiro) is a species of African bird of prey in the genus Accipiter which is the type genus of the family Accipitridae.\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "\n",
    "doc = nlp(\"The type species is Hombron's kingfisher (Actenoides hombroni)\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:12<00:00,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 19 pájaros distintos de 27. 27 encontrados con ner y 0 encontrados con entity ruler. Ha habido 6 falsos casos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hemos encontrado 'puna plover' con entrada en la dbpedia [Puna plover]'http://dbpedia.org/resource/Puna_plover'.\\n\",\n",
       " \"Hemos encontrado 'hawk' con entrada en la dbpedia [Hawk]'http://dbpedia.org/resource/Hawk'.\\n\",\n",
       " \"Hemos encontrado 'purple sandpiper' con entrada en la dbpedia [Purple sandpiper]'http://dbpedia.org/resource/Purple_sandpiper'.\\n\",\n",
       " \"Hemos encontrado 'solitary eagle' con entrada en la dbpedia [Solitary eagle]'http://dbpedia.org/resource/Solitary_eagle'.\\n\",\n",
       " \"Hemos encontrado 'cinclodes' con entrada en la dbpedia [Cinclodes]'http://dbpedia.org/resource/Cinclodes'.\\n\"]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "found_with_ruler = 0\n",
    "found_with_ner = 0\n",
    "error = 0\n",
    "maybe_matches = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  # if len(doc.ents) > 0:\n",
    "  #   displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == \"BIRD\":\n",
    "      (valid, alt_key, accuracy) = search_bird_dict(str(ent))\n",
    "      if valid:\n",
    "        found_with_ruler += 1\n",
    "        maybe_matches.append((ent.ent_id_, str(ent)))\n",
    "      else:\n",
    "        error += 1\n",
    "    else:\n",
    "      (valid, alt_key, accuracy) = search_bird_dict(str(ent))\n",
    "      if valid:\n",
    "        found_with_ner += 1\n",
    "        maybe_matches.append((ent.label_, str(ent)))\n",
    "      else:\n",
    "        error += 1\n",
    "\n",
    "# Pintar y guardar resultado\n",
    "result_lines = []\n",
    "for bird_key, original in set(maybe_matches):\n",
    "  bird = dict(birds[bird_key])\n",
    "  name = bird[\"name\"]\n",
    "  url = bird[\"url\"]\n",
    "  result_lines.append(f\"Hemos encontrado '{original}' con entrada en la dbpedia [{name}]'{url}'.\")\n",
    "\n",
    "\n",
    "result_lines_file = write(data_results_path, \"shorebirder_results_5\", result_lines)\n",
    "print(f\"Hemos encontrado {len(result_lines)} pájaros distintos de {len(maybe_matches)}.\",\n",
    "  f\"{found_with_ner} encontrados con ner y {found_with_ruler} encontrados con entity ruler.\",\n",
    "  f\"Ha habido {error} falsos casos\")\n",
    "open(result_lines_file, \"r\", encoding=\"utf-8\").readlines()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo entrenado es peor que en el apartado 4. (se ha añadido una validación porque daba muchos casos falsos)\n",
    "\n",
    "Se ha probado de unir la pipeline del apartado 4, `entity_ruler` (código comentado 2 bloques antes) y `ner` para mejorar los resultados y obtener contexto. Al no mejorarse los resultados del apartado 4 se ha abandonado el desarrollo dando el modelo del apartado 3 como el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El **intento 1** usando solamente los modelos pre-entrenados de spacy no son capaces de detectar aves. Previsible pero valía la pena intentar.\n",
    "\n",
    "El **intento 2** por planteamiento (y pista en el enunciado) es muy lento y se \"explota\" a la dbpedia a llamadas a base de datos para verificaciones simples. Tampoco podemos decir que sea una solución válida.\n",
    "\n",
    "El **intento 3** supone la primera solución válida al ejercicio, podría rebautizarse como *solución 1*. No solo descubre mayor número de pájaros sino que es \"preciso\". La pega que tiene su ejecución es la lentitud ya que requiere pasar pos el nlp de spacy para segmentar y encontrar entidades de segmento, y además, buscar si hay un nombre de pájaro realizando una comparación contra TODOS las posibles aves.\n",
    "\n",
    "El **intento 4** también solución válida mejora en eficiencia al intento 3. En este caso, sacrificamos precisión frente a rendimiento. El modelo es notablemente más rápido, pero no detecta variaciones. Una solución, podría ser alimentar a la pipe con distintas variaciones de los nombres de pájaros como añadir su variante al plural.\n",
    "\n",
    "El **intento 5**, posiblemente el más ambicioso y decepcionante a la vez, pretendía unir lo mejor de los intentos 3 y 4. Entrenar un modelo de clasificación. El modelo de clasificación consiste en entrenar una pipeline `ner` con los distintas frases donde \"manualmente\" le mostrásemos donde esta el nombre de cada pájaro y la etiqueta que le debe colocar. Además, para mejorar la detección se añadiría la pipeline del intento 4, y para obtener datos del contexto se añadiría una pipeline `ner` pre-entrenada. Las pruebas con 20-50 pájaros funcionaban increíblemente bien, tiempo de entreno largo, pero el modelo era bueno. Al entrenar con los 10369 pájaros el resultado fue muy distinto, el *entity_ruler* del intento 4 obtiene todos los match, y nuestro ner no consigue detectar nada. La guinda sobre el pastel es que nuestro ner \"obtiene\" pájaros que no existen.\n",
    "Supongo que la peor parte fue darse cuenta que tras 16h de \"training\" el modelo siguiera siendo menos efectivo que las dos soluciones anteriores estas siendo más \"simples\"."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
