{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Avistamiento de aves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos (scraping)\n",
    "\n",
    "Primero vamos a obtener los datos. Para ello nos visitaremos las web de avistamientos de aves [shorebirder](https://www.shorebirder.com/), [trevorsbirding](https://www.trevorsbirding.com/) y [dantallmansbirdblog](https://dantallmansbirdblog.blogspot.com/).\n",
    "\n",
    "Durante la visita a la web y haciendo uso del inspector (F12) podemos ver que las descripciones que necesitamos se encuentran en los tag de párrafo (entre *\\<p\\> TEXTO \\</p\\>*). Sabiendo eso vamos a crear funciones de utilidad que se encargarán de descargar el contenido de la web y extraer el texto.\n",
    "\n",
    "Las descargas las realizaremos en `data/raw` mientras que en `data/posts` guardaremos los textos encontrados.\n",
    "\n",
    "** `dantallmansbirdblog` tiene una estructura ligeramente diferente (entre *\\<p\\>\\</p\\> TEXTO \\<p\\>\\</p\\>*), a lo que tendremos que modificar la función `get_texts` (a continuación) para obtener sus textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import wget\n",
    "\n",
    "data_posts_cache = \"../data/cache\" # guardar resultados de queries a sparql\n",
    "data_raw_path = \"../data/raw\" # descargas\n",
    "data_posts_path = \"../data/posts\" # guardar los textos de los post scrapeados\n",
    "\n",
    "def maybe_mkdir(path):\n",
    "  try:\n",
    "    os.mkdir(path)\n",
    "  except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "maybe_mkdir(\"../data\")\n",
    "maybe_mkdir(data_raw_path)\n",
    "maybe_mkdir(data_posts_path)\n",
    "\n",
    "def download(url, out_label):\n",
    "  return wget.download(url, out=f\"{data_raw_path}/{out_label}\")\n",
    "\n",
    "def get_texts(filename):\n",
    "  file = open(filename, 'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "\n",
    "  # get texts\n",
    "  get_p = re.compile(r'<p>((.|\\n)*?)</p>')\n",
    "  texts = get_p.findall(text)\n",
    "\n",
    "  # remove styling and inner tags\n",
    "  remove_tags = re.compile(r'(<.*?>)|\\\\n| +(?= )|\\\\|\\&.+?\\;')\n",
    "  return map(lambda text: re.sub(remove_tags, \"\", str(text[0]).lower()), texts)\n",
    "\n",
    "def write(path, filename, data):\n",
    "  filepath = f\"{path}/{filename}.txt\"\n",
    "  file = open(filepath, \"a\", encoding=\"utf-8\")\n",
    "  for item in data:\n",
    "    file.write(str(item)+\"\\n\")\n",
    "  file.close()\n",
    "  return filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación haciendo uso de las funciones anteriores scrapeamos la home de `shorebirder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorebirder_filename = \"shorebirder_home.html\"\n",
    "shorebirder_home = download(\"https://www.shorebirder.com/\", shorebirder_filename)\n",
    "posts = get_texts(shorebirder_home)\n",
    "shorebirder_posts_file = write(data_posts_path, shorebirder_filename, posts)\n",
    "\n",
    "open(shorebirder_posts_file, \"r\").readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para `trevorsbirding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trevorsbirding_filename = \"trevorsbirding_home.html\"\n",
    "trevorsbirding_home = download(\"https://www.trevorsbirding.com/\", trevorsbirding_filename)\n",
    "posts = get_texts(trevorsbirding_home)\n",
    "trevorsbirding_posts_file = write(data_posts_path, trevorsbirding_filename, posts)\n",
    "\n",
    "open(trevorsbirding_posts_file, \"r\").readlines()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizar artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dependencias para el nlp\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in open(shorebirder_posts_file, \"r\").readlines()[3:5]:\n",
    "  train = nlp(text)\n",
    "  displacy.render(train, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este nlp no nos sirve o bien nos falta entrenarlo. No es capaz de encontrar los nombres de pájaros.\n",
    "\n",
    "vamos a probar de entrenarlo usando nombres de pájaros de la dbpedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixes and Class based from https://github.com/ejrav/pydbpedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class SparqlEndpoint(object):\n",
    "\n",
    "    def __init__(self, endpoint, prefixes={}):\n",
    "        self.sparql = SPARQLWrapper(endpoint)\n",
    "        self.prefixes = {\n",
    "            \"dbo\": \"http://dbpedia.org/ontology/\",\n",
    "            \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "            \"dbpedia2\": \"http://dbpedia.org/property/\",\n",
    "            \"dbpedia\": \"http://dbpedia.org/\",\n",
    "            \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"yago\": \"http://dbpedia.org/class/yago/\",\n",
    "            }\n",
    "        self.prefixes.update(prefixes)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "\n",
    "    def query(self, q):\n",
    "        lines = [\"PREFIX %s: <%s>\" % (k, r) for k, r in self.prefixes.items()]\n",
    "        lines.extend(q.split(\"\\n\"))\n",
    "        query = \"\\n\".join(lines)\n",
    "        self.sparql.setQuery(query)\n",
    "        results = self.sparql.query().convert()\n",
    "        return results[\"results\"][\"bindings\"]\n",
    "\n",
    "\n",
    "class DBpediaEndpoint(SparqlEndpoint):\n",
    "    def __init__(self, endpoint, prefixes = {}):\n",
    "        super(DBpediaEndpoint, self).__init__(endpoint, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = DBpediaEndpoint(endpoint = \"http://dbpedia.org/sparql\")\n",
    "\n",
    "# birds = s.query(\"\"\"\n",
    "#   SELECT DISTINCT *\n",
    "#   WHERE {\n",
    "#     ?bird a dbo:Bird ;\n",
    "#           a yago:Bird101503061 ;\n",
    "#           rdfs:label ?name ; # inicialmente esta propiedad era para evitar coger eventos y años en la busqueda\n",
    "#           dbo:abstract ?comment .\n",
    "\n",
    "#     filter (!isLiteral(?name) ||\n",
    "#             langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "#     filter (!isLiteral(?comment) ||\n",
    "#             langmatches(lang(?comment), \"en\")) .\n",
    "    \n",
    "#   }\n",
    "#   limit 10000\n",
    "# \"\"\")\n",
    "\n",
    "birds = s.query(\"\"\"\n",
    "  SELECT DISTINCT *\n",
    "  WHERE {\n",
    "    ?bird a dbo:Bird ;\n",
    "          rdfs:label ?name ;\n",
    "          dbo:abstract ?comment .\n",
    "\n",
    "    filter (!isLiteral(?name) ||\n",
    "            langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "    filter (!isLiteral(?comment) ||\n",
    "            langmatches(lang(?comment), \"en\")) .\n",
    "    \n",
    "  }\n",
    "  limit 10000\n",
    "\"\"\")\n",
    "\n",
    "birds += s.query(\"\"\"\n",
    "  SELECT DISTINCT *\n",
    "  WHERE {\n",
    "    ?bird a dbo:Bird ;\n",
    "          rdfs:label ?name ;\n",
    "          dbo:abstract ?comment .\n",
    "\n",
    "    filter (!isLiteral(?name) ||\n",
    "            langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "    filter (!isLiteral(?comment) ||\n",
    "            langmatches(lang(?comment), \"en\")) .\n",
    "    \n",
    "  }\n",
    "  limit 10000\n",
    "  offset 10000\n",
    "\"\"\")\n",
    "\n",
    "write(\"../data\", \"birds\", birds)\n",
    "\n",
    "print(f\"Hemos obtenido los nombres de {len(birds)} pájaros\")\n",
    "for d in birds[0:5]:\n",
    "  print(d['name']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# build training sentences\n",
    "training_data = [\n",
    "  # (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]), # example\n",
    "]\n",
    "\n",
    "tag = \"BIRD\"\n",
    "\n",
    "remove_parentesis_text = re.compile(r'\\(.*\\)')\n",
    "\n",
    "# mejorable pero hace su trabajo\n",
    "for bird in birds:\n",
    "  bird_name = bird['name']['value']\n",
    "  if len(bird_name) == 0:\n",
    "    continue\n",
    "  \n",
    "  bird_name = re.sub(remove_parentesis_text, \"\", str(bird_name).lower())\n",
    "  train_sentence = bird['comment']['value']\n",
    "  try:\n",
    "    start_at = strip_accents(train_sentence).lower().index(strip_accents(bird_name))\n",
    "    ends_at = start_at + len(bird_name)\n",
    "    training_data.append(\n",
    "      (bird_name, [(0, len(bird_name), tag)])\n",
    "    )\n",
    "    training_data.append(\n",
    "      (train_sentence, [(start_at, ends_at, tag)])\n",
    "    )\n",
    "  except:\n",
    "    # print(\"Error: substing not found\")\n",
    "    # print(bird_name)\n",
    "    # print(train_sentence)\n",
    "    # print(\"-\"*10)\n",
    "    training_data.append(\n",
    "      (bird_name, [(0, len(bird_name), tag)])\n",
    "    )\n",
    "\n",
    "def outer_join(lst1, lst2):\n",
    "  lst3 = [value for value in lst1 if not value in lst2]\n",
    "  return lst3\n",
    "\n",
    "test_data = random.sample(training_data, k=round(len(training_data)*0.10))\n",
    "training_data = outer_join(training_data, test_data)\n",
    "\n",
    "for text, annotations in training_data[0:5]:\n",
    "  print(text)\n",
    "  print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "skips = 0\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for text, annotations in training_data:\n",
    "  doc = nlp(text)\n",
    "  ents = []\n",
    "  for start, end, label in annotations:\n",
    "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "    if span is None:\n",
    "      skips += 1\n",
    "    else:\n",
    "      ents.append(span)\n",
    "  filtered_ents = filter_spans(ents)\n",
    "  doc.ents = filtered_ents\n",
    "  db.add(doc)\n",
    "\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "skips = 0\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for text, annotations in test_data:\n",
    "  doc = nlp(text)\n",
    "  ents = []\n",
    "  for start, end, label in annotations:\n",
    "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "    if span is None:\n",
    "      skips += 1\n",
    "    else:\n",
    "      ents.append(span)\n",
    "  filtered_ents = filter_spans(ents)\n",
    "  doc.ents = filtered_ents\n",
    "  db.add(doc)\n",
    "\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "!python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "\n",
    "# dev.spacy it's the validation set ... windows users better use powershell... (\"cd src\" and run the comand without !)\n",
    "!python -m spacy train config.cfg --output ./output --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nlp = spacy.load(\"./output/model-best\")\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = my_nlp(text)\n",
    "  if len(doc.ents) > 0:\n",
    "    displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fracaso de nlp... ultimo intento usando spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bird_dbpedia(token):\n",
    "  return s.query('''\n",
    "    SELECT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "\n",
    "      filter (CONTAINS(LCASE(STR(?name)), \"{token}\")) .\n",
    "    }\n",
    "    limit 5\n",
    "  '''.replace(\"{token}\", token))\n",
    "\n",
    "search_bird_dbpedia(\"falcon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_matches = {}\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in doc.noun_chunks:\n",
    "    for token in chunk:\n",
    "      if token.pos_ == 'NOUN':\n",
    "        results = search_bird_dbpedia(token.lemma_)\n",
    "        if len(results) > 0:\n",
    "          maybe_matches[token] = results\n",
    "          print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, mejor. Lo que es muuuy lento y estamos machacando la dbpedia a queries. Vamos a mezclar ambas estrategias.\n",
    "\n",
    "Del intento 1 vamos a coger los resultados de todos los pájaros y lo convertiremos en un diccionario para que nos sea más fácil buscar.\n",
    "Con spacy solo tokenizaremos y obtendremos los `noun chunk` y usando fuzzy search buscaremos las aves.\n",
    "Para hacer spacy más rápido vamos a deshabilitar las pipelines que no usemos, que son `lemmatizer` y `ner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_dic = {}\n",
    "for bird in birds:\n",
    "  key = bird[\"name\"][\"value\"].lower()\n",
    "  birds_dic[key] = {\n",
    "    \"name\": bird[\"name\"][\"value\"],\n",
    "    \"url\": bird[\"bird\"][\"value\"],\n",
    "    \"description\": bird[\"comment\"][\"value\"],\n",
    "  }\n",
    "birds_keys = birds_dic.keys() # buscaremos por las key\n",
    "assert len(birds) == len(birds_keys) # aseguramos que no haya ninguna key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deshabilitar pipes de spacy\n",
    "slim_nlp = spacy.load(\"en_core_web_lg\", disable=['lemmatizer', 'ner'])\n",
    "\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = slim_nlp(text)\n",
    "  displacy.render(doc, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer que el tokenizador no separe palabras con guion\n",
    "# https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "def custom_tokenizer(slim_nlp):\n",
    "    inf = list(slim_nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(slim_nlp.vocab, prefix_search=slim_nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=slim_nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=slim_nlp.tokenizer.token_match,\n",
    "                                rules=slim_nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "slim_nlp.tokenizer = custom_tokenizer(slim_nlp)\n",
    "\n",
    "# Test\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = slim_nlp(text)\n",
    "  displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "def search_bird_dict(chunk):\n",
    "  best_match = (False, None, 0)\n",
    "  for key in birds_keys:\n",
    "    score = fuzz.ratio(key, chunk)\n",
    "    if score > 80 and score > best_match[-1]:\n",
    "      # print(f\"\\tchunk: '{chunk}' compare '{key}' score: '{score}'\")\n",
    "      best_match = (True, key, score)\n",
    "  return best_match\n",
    "\n",
    "def get_nouns(chunk):\n",
    "  only_nouns = []\n",
    "  for token in chunk:\n",
    "    if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
    "      only_nouns.append(token.lower_)\n",
    "\n",
    "  if len(only_nouns) > 0:\n",
    "    return \" \".join(only_nouns)\n",
    "  return None\n",
    "\n",
    "maybe_matches = []\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = slim_nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "        print(f\"Found '{bird_key}' in '{chunk}' with score of {score}\")\n",
    "          \n",
    "# Test result\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos ajustado para que recoja los pájaros de esa frase. vamos a ver si puede coger los de las otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "maybe_matches = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = slim_nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lines = []\n",
    "for bird_key in set(maybe_matches):\n",
    "  bird = dict(birds_dic[bird_key])\n",
    "  name = bird[\"name\"]\n",
    "  url = bird[\"url\"]\n",
    "  result_lines.append(f\"Hemos encontrado '{name}' con entrada en la dbpedia '{url}'.\")\n",
    "\n",
    "result_lines_file = write(data_posts_path, \"shorebirder_results\", result_lines)\n",
    "open(result_lines_file, \"r\").readlines()[0:5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
