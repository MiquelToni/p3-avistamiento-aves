{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Avistamiento de aves\n",
    "\n",
    "- autor: Miquel Antoni Llambías Cabot\n",
    "- repository: [github](https://github.com/MiquelToni/p3-avistamiento-aves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos (scraping)\n",
    "\n",
    "Primero vamos a obtener los datos. Para ello nos visitaremos las web de avistamientos de aves [shorebirder](https://www.shorebirder.com/), [trevorsbirding](https://www.trevorsbirding.com/) y [dantallmansbirdblog](https://dantallmansbirdblog.blogspot.com/).\n",
    "\n",
    "Durante la visita a la web y haciendo uso del inspector (F12) podemos ver que las descripciones que necesitamos se encuentran en los tag de párrafo (entre *\\<p\\> TEXTO \\</p\\>*). Sabiendo eso vamos a crear funciones de utilidad que se encargarán de descargar el contenido de la web y extraer el texto.\n",
    "\n",
    "Las descargas las realizaremos en `data/raw` mientras que en `data/posts` guardaremos los textos encontrados.\n",
    "\n",
    "** `dantallmansbirdblog` tiene una estructura ligeramente diferente (entre *\\<p\\>\\</p\\> TEXTO \\<p\\>\\</p\\>*), a lo que tendremos que modificar la función `get_texts` (a continuación) para obtener sus textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O utils\n",
    "import os\n",
    "from os.path import exists\n",
    "import re\n",
    "import wget\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "# auxiliar regex\n",
    "remove_parentesis_text = re.compile(r'\\(.*\\)')\n",
    "sentence_endings = re.compile(r'\\.|\\*')\n",
    "\n",
    "# auxiliar functions\n",
    "def maybe_mkdir(path):\n",
    "  try:\n",
    "    if not exists(path):\n",
    "      os.mkdir(path)\n",
    "  except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "def download(url, out_label):\n",
    "  filepath = f\"{data_raw_path}/{out_label}\"\n",
    "  if exists(filepath):\n",
    "    os.remove(filepath)\n",
    "  return wget.download(url, out=filepath)\n",
    "\n",
    "def get_texts(filename):\n",
    "  file = open(filename, 'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "\n",
    "  # get texts\n",
    "  get_p = re.compile(r'<p>((.|\\n)*?)</p>')\n",
    "  texts = get_p.findall(text)\n",
    "\n",
    "  # remove styling and inner tags\n",
    "  remove_tags = re.compile(r'(<.*?>)|\\\\n| +(?= )|\\\\|\\&.+?\\;')\n",
    "  return map(lambda text: re.sub(remove_tags, \"\", str(text[0]).lower()), texts)\n",
    "\n",
    "def write(path, filename, data):\n",
    "  filepath = f\"{path}/{filename}.txt\"\n",
    "  file = open(filepath, \"a\", encoding=\"utf-8\")\n",
    "  for item in data:\n",
    "    file.write(str(item)+\"\\n\")\n",
    "  file.close()\n",
    "  return filepath\n",
    "\n",
    "def save_json(path, filename, dict):\n",
    "  filepath = f\"{path}/{filename}.json\"\n",
    "  # create json object from dictionary\n",
    "  parsed_json = json.dumps(dict)\n",
    "  f = open(filepath,\"w\")\n",
    "  # write json object to file\n",
    "  f.write(parsed_json)\n",
    "  # close file\n",
    "  f.close()\n",
    "  return filepath\n",
    "\n",
    "# constants\n",
    "data_posts_cache = \"../data/cache\" # guardar resultados de queries a sparql\n",
    "data_raw_path = \"../data/raw\" # descargas\n",
    "data_posts_path = \"../data/posts\" # guardar los textos de los post scrapeados\n",
    "data_results_path = \"../data/results\" # guardar los resultados de las diferentes pruebas\n",
    "\n",
    "# build directory structure\n",
    "maybe_mkdir(\"../models\")\n",
    "maybe_mkdir(\"../models/entity_ruler\")\n",
    "maybe_mkdir(\"../data\")\n",
    "maybe_mkdir(\"../owl\")\n",
    "maybe_mkdir(data_raw_path)\n",
    "maybe_mkdir(data_posts_path)\n",
    "maybe_mkdir(data_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación haciendo uso de las funciones anteriores scrapeamos la home de `shorebirder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my late march solo visit to norway is in the books and was about as much fun as i've had in a while. the middle few days of the trip were spent birding around varanger, bookended by more touristy time intromsã¸and oslo. at some point in the coming months there will be a full trip report here plus a very detailed cloudbirders submission. in the meantime, here is some proof that i actually went.\\n\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrap shorebirder.com\n",
    "reviews_filename = \"reviews\"\n",
    "shorebirder_filename = \"shorebirder_home.html\"\n",
    "shorebirder_home = download(\"https://www.shorebirder.com/\", shorebirder_filename)\n",
    "posts = get_texts(shorebirder_home)\n",
    "shorebirder_posts_file = write(data_posts_path, reviews_filename, posts)\n",
    "\n",
    "open(shorebirder_posts_file, \"r\", encoding=\"utf-8\").readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para `trevorsbirding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i kid. new haven gets a bad rap, and it really shouldn\\'t. or is it \"bad rep??\" i just don\\'t know.\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrap trevorsbirding.com\n",
    "trevorsbirding_filename = \"trevorsbirding_home.html\"\n",
    "trevorsbirding_home = download(\"https://www.trevorsbirding.com/\", trevorsbirding_filename)\n",
    "posts = get_texts(trevorsbirding_home)\n",
    "trevorsbirding_posts_file = write(data_posts_path, reviews_filename, posts)\n",
    "\n",
    "open(trevorsbirding_posts_file, \"r\").readlines()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se han obtenido las descripciones de `dantallmansbirdblog` dado que con los dos primeros ya cubrimos el \"mínimo\" frases de posibles avistamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 1: Usar spacy sin modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dependencias para el nlp\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    new haven\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ": come for the pizza, stay for the crime!</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    last week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " i was out messing with my brand new camera body, the canon \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    r5\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ", which at the time i had paired with my trusty ol' \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    400mm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " f5.6. i was walking back to the car at \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    fort hale park\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " when i heard a &quot;thwack!&quot; right next to me followed by the most awful gull scream you've ever heard. i looked down to my left and a ring-billed gull was flailing a bloody, broken right wing. looking up i eventually located the culprit, a hefty adult (presumed) female peregrine falcon.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prueba\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[3:5]:\n",
    "  train = nlp(text)\n",
    "  displacy.render(train, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es capaz de identificar diferentes entidades dentro de las frases, pero no pájaros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 2: Usando sparql query para encontrar aves\n",
    "\n",
    "Idea: Usando el tokenizer de spacy como tokenizer trocear las frases. A partir de los token etiquetados como nombre (`NOUN`) lanzamos una petición a la dbpedia. Ya el resultado de la dbpedia nos dirá si existe y cual es su etiqueta / url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparQL class extension\n",
    "# Prefixes and Class based from https://github.com/ejrav/pydbpedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class SparqlEndpoint(object):\n",
    "\n",
    "    def __init__(self, endpoint, prefixes={}):\n",
    "        self.sparql = SPARQLWrapper(endpoint)\n",
    "        self.prefixes = {\n",
    "            \"dbo\": \"http://dbpedia.org/ontology/\",\n",
    "            \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "            \"dbpedia2\": \"http://dbpedia.org/property/\",\n",
    "            \"dbpedia\": \"http://dbpedia.org/\",\n",
    "            \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"yago\": \"http://dbpedia.org/class/yago/\",\n",
    "            }\n",
    "        self.prefixes.update(prefixes)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "\n",
    "    def query(self, q):\n",
    "        lines = [\"PREFIX %s: <%s>\" % (k, r) for k, r in self.prefixes.items()]\n",
    "        lines.extend(q.split(\"\\n\"))\n",
    "        query = \"\\n\".join(lines)\n",
    "        self.sparql.setQuery(query)\n",
    "        results = self.sparql.query().convert()\n",
    "        return results[\"results\"][\"bindings\"]\n",
    "\n",
    "\n",
    "class DBpediaEndpoint(SparqlEndpoint):\n",
    "    def __init__(self, endpoint, prefixes = {}):\n",
    "        super(DBpediaEndpoint, self).__init__(endpoint, prefixes)\n",
    "\n",
    "s = DBpediaEndpoint(endpoint = \"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para buscar una ave dado su nombre\n",
    "def search_bird_dbpedia(token):\n",
    "  return s.query('''\n",
    "    SELECT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "\n",
    "      filter (CONTAINS(LCASE(STR(?name)), \"{token}\")) .\n",
    "    }\n",
    "    limit 5\n",
    "  '''.replace(\"{token}\", token))\n",
    "\n",
    "search_bird_dbpedia(\"falcon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "time\n",
      "ol'\n",
      "car\n",
      "gull\n",
      "ring\n",
      "gull\n",
      "wing\n",
      "peregrine\n",
      "falcon\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "maybe_matches = {}\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in doc.noun_chunks:\n",
    "    for token in chunk:\n",
    "      if token.pos_ == 'NOUN':\n",
    "        results = search_bird_dbpedia(token.lemma_)\n",
    "        if len(results) > 0:\n",
    "          maybe_matches[token] = results\n",
    "          print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, funciona? Lo que es muy lento y estamos machacando la dbpedia a queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 3: Cachear / indexar la dbpedia\n",
    "\n",
    "Del intento anterior vamos a coger los resultados de todos los pájaros y lo convertiremos en un diccionario para que nos sea más fácil buscar y solo haremos n queries a la dbpedia. Por supuesto, esta estrategia es solo factible si el conjunto es finito. Como es nuestro caso, va haber n especies de pájaros, pero no va a estar creciendo dia a dia.\n",
    "\n",
    "### Estrategia\n",
    "- Obtener lista de todos los nombres de pájaros.\n",
    "- Con spacy analizaremos la entrada del avistamiento y obtenemos los `noun chunk` y `ents`.\n",
    "  - Para obtener los chunk necesitamos las pipelines de `tok2vec`, `tagger`, `parser` y `attribute_ruler`.\n",
    "  - Para obtener las entidades necesitamos `ner`.\n",
    "- Con cada `chunk` usando fuzzy-search en la lista de nombres de pájaros para encontrar aquellos chunk que parezcan nombres de pájaros.\n",
    "- Con cada `ent` nos aportará datos sobre el contexto: ubicación, fechas u otros. Este ent se asociará al pájaro si el ent pertenece al chunk del pájaro y en su defecto al avistamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos obtenido los nombres de 10369 pájaros\n",
      "Actenoides\n",
      "African goshawk\n",
      "African pitta\n",
      "African red-eyed bulbul\n",
      "Alcedo\n"
     ]
    }
   ],
   "source": [
    "# Obtener todos los pájaros con nombre, url y descripción\n",
    "def get_all_birds_sparql(batch_size):\n",
    "  found_all = False\n",
    "  limit = batch_size\n",
    "  offset = 0\n",
    "  query = \"\"\"\n",
    "    SELECT DISTINCT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "      \n",
    "    }\n",
    "    limit {limit_value}\n",
    "    offset {offset_value}\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  while not found_all:\n",
    "    loop_result = s.query(query.replace(\"{limit_value}\", str(limit)).replace(\"{offset_value}\", str(offset)))\n",
    "    if len(loop_result) < limit:\n",
    "      found_all = True\n",
    "    else:\n",
    "      offset = limit\n",
    "      limit += batch_size\n",
    "    result += loop_result\n",
    "  return result\n",
    "\n",
    "birds_sparql = get_all_birds_sparql(5000)\n",
    "\n",
    "write(\"../data\", \"birds\", birds_sparql)\n",
    "\n",
    "print(f\"Hemos obtenido los nombres de {len(birds_sparql)} pájaros\")\n",
    "for d in birds_sparql[0:5]:\n",
    "  print(d['name']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los datos en crudo a un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparql a diccionario\n",
    "remove_parentesis_text = re.compile(r'\\(|\\)')\n",
    "\n",
    "birds = {}\n",
    "for bird in birds_sparql:\n",
    "  bird_name_lower = bird[\"name\"][\"value\"].lower()\n",
    "  key = re.sub(remove_parentesis_text, \"\", str(bird_name_lower))\n",
    "  if key in birds.keys():\n",
    "    print(f\"bird {key} is duplicated.\", birds[key][\"name\"], bird[\"name\"][\"value\"])\n",
    "  birds[key] = {\n",
    "    \"name\": bird[\"name\"][\"value\"],\n",
    "    \"url\": bird[\"bird\"][\"value\"],\n",
    "    \"description\": bird[\"comment\"][\"value\"],\n",
    "  }\n",
    "bird_keys = birds.keys() # buscaremos por las key\n",
    "assert len(birds_sparql) == len(bird_keys) # aseguramos que no hayamos perdido ninguna key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Goliath (Mangalia)',\n",
       " 'url': 'http://dbpedia.org/resource/Goliath_(Mangalia)',\n",
       " 'description': 'Goliath is the name of a crane that is currently located at the Mangalia shipyard in Mangalia, Romania. Formerly, it was part of the Fore River Shipyard in Quincy, Massachusetts.'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds[\"goliath mangalia\"] # bueno... la dbpedia tampoco es perfecta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos el `nlp`. Usamos el preset large (`en_core_web_lg`) porque nos proporciona más precisión en el reconocimiento de entidades. Este reconocimiento de entidades lo usaremos para registrar elementos del contexto y completar los individuos de nuestra ontología"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7828ec90aaa74e22a8fae7998742089d-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Black-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">billed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">flycatcher</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7828ec90aaa74e22a8fae7998742089d-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7828ec90aaa74e22a8fae7998742089d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7828ec90aaa74e22a8fae7998742089d-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7828ec90aaa74e22a8fae7998742089d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preparar nlp\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['lemmatizer'])\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el `tokenizer` de spacy nos separa las palabras compuestas con guion.\n",
    "\n",
    "Para solucionarlo vamos a modificar el tokenizer para que no separe las palabras con guion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer que el tokenizer no separe palabras con guion\n",
    "# https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3b359771e7e94333bdab101968140101-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Black-billed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">flycatcher</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b359771e7e94333bdab101968140101-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b359771e7e94333bdab101968140101-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genial! A demás conseguimos que `Black-billed` se detecte como adjetivo y no como adjetivo + verbo.\n",
    "\n",
    "A continuación, creamos las funciones necesarias para detectar los pájaros y una demo/test del funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'ring-billed gull' in 'ring-billed gull' with score of 100\n",
      "Found 'peregrine falcon' in 'female peregrine falcon' with score of 82\n",
      "['ring-billed gull', 'peregrine falcon']\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de pájaros por fuzzysearch\n",
    "from fuzzywuzzy import fuzz\n",
    "def search_bird_dict(chunk):\n",
    "  best_match = (False, None, 0)\n",
    "  for key in bird_keys:\n",
    "    score = fuzz.ratio(key, chunk)\n",
    "    if score > best_match[-1]:\n",
    "      # print(f\"\\tchunk: '{chunk}' compare '{key}' score: '{score}'\")\n",
    "      best_match = (score > 80, key, score)\n",
    "  return best_match\n",
    "\n",
    "def get_nouns(chunk):\n",
    "  only_nouns = []\n",
    "  for token in chunk:\n",
    "    if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
    "      only_nouns.append(token.lower_)\n",
    "\n",
    "  if len(only_nouns) > 0:\n",
    "    return \" \".join(only_nouns)\n",
    "  return None\n",
    "\n",
    "maybe_matches = []\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[0:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "  # for chunk in doc.noun_chunks: # aplicar sugerencia a ver si es más rápido así -- 1.0s +/- 0.1s más lento que el anterior -- no y perdemos precisión\n",
    "  #   chunk = str(chunk)\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "        print(f\"Found '{bird_key}' in '{chunk}' with score of {score}\")\n",
    "          \n",
    "# Test result\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el chunk siguiente, definimos como queremos que sea nuestra ontología.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos la ontología\n",
    "# pip install Cython==0.29.28 owlready2==0.37\n",
    "# pip install slugify\n",
    "from owlready2 import *\n",
    "from slugify import slugify\n",
    "import uuid\n",
    "\n",
    "# Creamos una ontología\n",
    "onto = get_ontology(\"http://avistamiento-aves-3.org/onto.owl\")\n",
    "onto.destroy()\n",
    "onto = get_ontology(\"http://avistamiento-aves-3.org/onto.owl\")\n",
    "\n",
    "with onto:\n",
    "  class label(DataProperty):\n",
    "    range = [str]\n",
    "  class text_field(DataProperty):\n",
    "    range = [str]\n",
    "  class match_text(DataProperty):\n",
    "    range = [str]\n",
    "  class match_score(DataProperty):\n",
    "    range = [float]\n",
    "  class dbpedia(DataProperty):\n",
    "    range = [str]\n",
    "  class date(DataProperty):\n",
    "    range = [str]\n",
    "  class position(DataProperty):\n",
    "    range = [str]\n",
    "  class generic_context(DataProperty):\n",
    "    range = [str]\n",
    "\n",
    "  class Bird(Thing):\n",
    "    pass\n",
    "  class Sighting(Thing):\n",
    "    pass\n",
    "\n",
    "  class has_been_seen(ObjectProperty):\n",
    "    domain = [Bird]\n",
    "    range = [Sighting]\n",
    "  class birds_found(ObjectProperty):\n",
    "    domain = [Sighting]\n",
    "    range = [Bird]\n",
    "    inverse_property = has_been_seen\n",
    "\n",
    "# Owl functions\n",
    "def create_sighting(doc):\n",
    "  sighting = Sighting(f\"sighting-{uuid.uuid4()}\")\n",
    "  sighting.text_field.append(doc.text)\n",
    "  return sighting\n",
    "\n",
    "def register_bird(sighting, bird_key, chunk_nouns, score):\n",
    "  slugify_name = slugify(bird_key, separator=\"_\")\n",
    "  bird_onto = types.new_class(slugify_name, (Bird,))\n",
    "  bird_dict = birds[bird_key]\n",
    "  bird_individual = bird_onto(f\"{slugify_name}-{uuid.uuid4()}\")\n",
    "  bird_individual.label.append(bird_dict[\"name\"])\n",
    "  bird_individual.match_text.append(chunk_nouns)\n",
    "  bird_individual.match_score.append(score)\n",
    "  bird_individual.dbpedia.append(bird_dict[\"url\"])\n",
    "  sighting.birds_found.append(bird_individual)\n",
    "  return bird_individual\n",
    "\n",
    "def add_context(individual, name, type):\n",
    "  if type == \"DATE\":\n",
    "    individual.date.append(name)\n",
    "  if type == \"NORP\" or type == \"GPE\":\n",
    "    individual.position.append(name)\n",
    "  else:\n",
    "    individual.generic_context.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:46<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 87 pájaros de 71 diferentes tipos\n",
      "../data/results/shorebirder_results_log_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Found 'ring-billed gull' in 'ring-billed gull' with score of 100\\n\",\n",
       " \"Found 'peregrine falcon' in 'female peregrine falcon' with score of 82\\n\"]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "trace = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  sighting = create_sighting(doc)\n",
    "\n",
    "  for chunk in doc.noun_chunks:\n",
    "    chunk_nouns = get_nouns(chunk)\n",
    "    if chunk_nouns != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk_nouns)\n",
    "      if found:\n",
    "        bird_individual = register_bird(sighting, bird_key, chunk_nouns, score)\n",
    "        trace.append(f\"Found '{bird_key}' in '{chunk_nouns}' with score of {score}\")\n",
    "        for ent in chunk.ents:\n",
    "          add_context(bird_individual, str(ent), ent.label_)\n",
    "      else:\n",
    "        for ent in chunk.ents:\n",
    "          add_context(sighting, str(ent), ent.label_)\n",
    "\n",
    "onto.save(file=\"../owl/avistamiento-aves-3.xml\", format = \"rdfxml\")\n",
    "log_result = write(data_results_path, \"shorebirder_results_log_3\", trace)\n",
    "print(f\"Hemos encontrado {len(trace)} pájaros de {len(list(Bird.subclasses()))} diferentes tipos\")\n",
    "print(log_result)\n",
    "open(log_result, \"r\", encoding=\"utf-8\").readlines()[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera versión funcional sin machacar a la dbpedia. Pero sigue siendo muy lento. Vamos a seguir probando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nueva pipeline [entity_ruler](https://spacy.io/api/entityruler)\n",
    "\n",
    "En esta aproximación vamos a añadir una pipe más al nlp `en_core_web_lg` pre-entrenado de spacy. Para ello necesitamos hacer una lista de todos los patterns que queramos poner. Es decir, debemos introducir los nombres de los pájaros que queremos que se detecten como patterns y añadir la nueva pipe al nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando entity_ruler https://spacy.io/usage/rule-based-matching#entityruler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "tag = \"BIRD\"\n",
    "\n",
    "# init blank nlp\n",
    "nlp = English()\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# Añadir los nombres de pájaros\n",
    "patterns = []\n",
    "for key in bird_keys:\n",
    "  bird = birds[key]\n",
    "  doc = nlp(bird[\"name\"])\n",
    "  pattern = []\n",
    "  for token in doc:\n",
    "    pattern.append({\n",
    "      \"LOWER\": token.lower_\n",
    "    })\n",
    "\n",
    "  patterns.append({\n",
    "    \"label\": tag,\n",
    "    \"pattern\": pattern,\n",
    "    \"id\": key\n",
    "  })\n",
    "\n",
    "# add entity_ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "with nlp.select_pipes(enable=\"tagger\"):\n",
    "  ruler.add_patterns(patterns)\n",
    "\n",
    "nlp.to_disk(\"../models/entity_ruler/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">that feeding \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gull\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BIRD</span>\n",
       "</mark>\n",
       " flock continued to produce by sucking in passers by. at one point a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bonaparte's gull\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BIRD</span>\n",
       "</mark>\n",
       " got in on the action, and a flock of 21 common terns appeared from the east and eventually settled into that flock.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity_ruler']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"that feeding gull flock continued to produce by sucking in passers by. at one point a Bonaparte's gull got in on the action, and a flock of 21 common terns appeared from the east and eventually settled into that flock.\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clonamos el schema de la ontología del apartado anterior. A esta, tenemos que modificar la función `register_bird` porque en este caso no tenemos `name_chunks` ni un `score` de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos la ontología\n",
    "from owlready2 import *\n",
    "from slugify import slugify\n",
    "import uuid\n",
    "\n",
    "# Creamos una ontología\n",
    "onto = get_ontology(\"http://avistamiento-aves-4.org/onto.owl\")\n",
    "onto.destroy()\n",
    "onto = get_ontology(\"http://avistamiento-aves-4.org/onto.owl\")\n",
    "\n",
    "with onto:\n",
    "  class label(DataProperty):\n",
    "    range = [str]\n",
    "  class text_field(DataProperty):\n",
    "    range = [str]\n",
    "  class match_text(DataProperty):\n",
    "    range = [str]\n",
    "  class match_score(DataProperty):\n",
    "    range = [float]\n",
    "  class dbpedia(DataProperty):\n",
    "    range = [str]\n",
    "  class date(DataProperty):\n",
    "    range = [str]\n",
    "  class position(DataProperty):\n",
    "    range = [str]\n",
    "  class generic_context(DataProperty):\n",
    "    range = [str]\n",
    "\n",
    "  class Bird(Thing):\n",
    "    pass\n",
    "  class Sighting(Thing):\n",
    "    pass\n",
    "\n",
    "  class has_been_seen(ObjectProperty):\n",
    "    domain = [Bird]\n",
    "    range = [Sighting]\n",
    "  class birds_found(ObjectProperty):\n",
    "    domain = [Sighting]\n",
    "    range = [Bird]\n",
    "    inverse_property = has_been_seen\n",
    "\n",
    "# Owl functions\n",
    "def create_sighting(doc):\n",
    "  sighting = Sighting(f\"sighting-{uuid.uuid4()}\")\n",
    "  sighting.text_field.append(doc.text)\n",
    "  return sighting\n",
    "\n",
    "def register_bird(sighting, bird_key):\n",
    "  slugify_name = slugify(bird_key, separator=\"_\")\n",
    "  bird_onto = types.new_class(slugify_name, (Bird,))\n",
    "  bird_dict = birds[bird_key]\n",
    "  bird_individual = bird_onto(f\"{slugify_name}-{uuid.uuid4()}\")\n",
    "  bird_individual.label.append(bird_dict[\"name\"])\n",
    "  bird_individual.dbpedia.append(bird_dict[\"url\"])\n",
    "  sighting.birds_found.append(bird_individual)\n",
    "  return bird_individual\n",
    "\n",
    "def add_context(individual, name, type):\n",
    "  if type == \"DATE\":\n",
    "    individual.date.append(name)\n",
    "  if type == \"NORP\" or type == \"GPE\":\n",
    "    individual.position.append(name)\n",
    "  else:\n",
    "    individual.generic_context.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:01<00:00, 59.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 52 pájaros de 45 diferentes tipos\n",
      "../data/posts/shorebirder_results_log_4.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Found 'gull'\\n\",\n",
       " \"Found 'ring-billed gull'\\n\",\n",
       " \"Found 'peregrine falcon'\\n\",\n",
       " \"Found 'gull'\\n\",\n",
       " \"Found 'gull'\\n\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "nlp = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "trace = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  sighting = create_sighting(doc)\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == tag and ent.ent_id_ != \"\":\n",
    "      bird_individual = register_bird(sighting, ent.ent_id_)\n",
    "      trace.append(f\"Found '{ent.ent_id_}'\")\n",
    "    else:\n",
    "      add_context(sighting, str(ent), ent.label_)\n",
    "\n",
    "onto.save(file=\"../owl/avistamiento-aves-4.xml\", format = \"rdfxml\")\n",
    "log_result = write(data_posts_path, \"shorebirder_results_log_4\", trace)\n",
    "print(f\"Hemos encontrado {len(trace)} pájaros de {len(list(Bird.subclasses()))} diferentes tipos\")\n",
    "print(log_result)\n",
    "open(log_result, \"r\", encoding=\"utf-8\").readlines()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejoramos mucho la velocidad por una cantidad aceptable de reconocimiento de aves. Pero no nos detecta todas las que conseguimos detectar en el `intento 3`. Esto es porque la capa de entity_ruler no es parte del modelo sino pattern matching. por ello no es capaz de encontrar las formas en plural de las aves que el intento 3 si encuentra.\n",
    "Al tampoco usar `ner` perdemos la capacidad de detectar elementos del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenar ner Spacy\n",
    "\n",
    "En este apartado vamos a centrarnos en mejorar la detección de spacy entrenando un modelo de clasificación. Las frases de entreno las cogeremos de las descripciones de la dbpedia. Los datos de entreno deberán estar compuestos de una frase más las posiciones de los nombres de pájaros. Para reconocer los pájaros que aparecen en las descripciones usaremos el `entity ruler` del apartado anterior. Al usar el entity ruler para esta tarea vamos a suponer que en la dbpedia no hay faltas de ortografía en las descripciones para que los pájaros sean identificables, y su nombre aparezca al menos una vez.\n",
    "\n",
    "La [pipeline](https://spacy.io/usage/spacy-101#pipelines) de spacy que se encarga de etiquetar las entities se llama [Entity Recognizer](https://spacy.io/api/entityrecognizer) o `ner`. Según la documentación de spacy sobre como [entrenar](https://spacy.io/usage/training) un modelo, primero necesitamos las frases de entreno correctamente etiquetadas, vectores por palabra a etiquetar y un fichero de configuración con las pipelines a entrenar.\n",
    "\n",
    "La generación de frases de entreno usaremos el 100% de frases para train y 20% para test (subconjunto de train). He decidido \"correr el riesgo\" de over-fitting del modelo a la de no registrar pájaros al modelo. He tomado esta decisión dada las escasas frases/pájaro que podemos obtener de la dbpedia.\n",
    "\n",
    "Para que ner pueda reconocer y comparar las aves vamos a tener que generar [vectores](https://spacy.io/usage/linguistic-features#vectors-similarity). Sabemos que el nlp de spacy no genera vectores para palabras fuera del modelo, o en el caso de `en_core_web_sm` directamente no tiene. Para ello, la generación de vectores se va a realizar mediante la librería [gensim](https://radimrehurek.com/gensim/) y la técnica [Word2Vec](https://es.wikipedia.org/wiki/Word2vec) como sugieren en la documentación de spacy.\n",
    "\n",
    "El fichero de configuración guardado en `src/birds_config.cfg` se ha generado usando la [herramienta web](https://spacy.io/usage/training#quickstart) que proporciona spacy seleccionando `ner` + `cpu` + `efficiency`. Ner, porque es la pipeline que queremos entrenar. Cpu, porque he tenido problemas para compilar pytorch para usar la gpu y es una opción más compatible. Efficiency porque nosotros le proporcionaremos los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\Anaconda3\\lib\\site-packages\\spacy\\language.py:710: UserWarning: [W113] Sourced component 'entity_ruler' may not work as expected: source vectors are not identical to current pipeline vectors.\n",
      "  warnings.warn(Warnings.W113.format(name=source_name))\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actenoides is a genus of kingfishers in the subfamily Halcyoninae [(0, 10, 'actenoides')]\n",
      "----------\n",
      " The genus Actenoides was introduced by the French ornithologist Charles Lucien Bonaparte in 1850 [(11, 21, 'actenoides')]\n",
      "----------\n",
      " The type species is Hombron's kingfisher (Actenoides hombroni) [(21, 41, \"hombron's kingfisher\"), (43, 53, 'actenoides')]\n",
      "----------\n",
      " A molecular study published in 2017 found that the genus Actenoides, as currently defined, is paraphyletic [(58, 68, 'actenoides')]\n",
      "----------\n",
      " The glittering kingfisher in the monotypic genus Caridonax is a member of the clade containing the species in the genus Actenoides [(5, 26, 'glittering kingfisher'), (121, 131, 'actenoides')]\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformar las descripciones en datos de entreno y test para spacy\n",
    "import gensim\n",
    "import random\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# init the \"base nlp\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# add custom tokenizer\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "# add entity_ruler \n",
    "entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.add_pipe(\"entity_ruler\", source=entity_ruler, before=\"ner\")\n",
    "\n",
    "def get_vector(ent):\n",
    "  if ent.has_vector:\n",
    "    return ent.vector\n",
    "  else:\n",
    "    return gensim.models.Word2Vec(str(ent), min_count = 1, window = 6, sg=0)\n",
    "\n",
    "# build training sentences\n",
    "training_data = [\n",
    "  # (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")], tag/label, vector), # example\n",
    "]\n",
    "for key in tqdm.tqdm(bird_keys):\n",
    "  bird = birds[key]\n",
    "  bird_name = bird[\"name\"]\n",
    "\n",
    "  if len(bird_name) == 0:\n",
    "    continue\n",
    "  \n",
    "  bird_name = re.sub(remove_parentesis_text, \"\", str(bird_name))\n",
    "  description = bird[\"description\"]\n",
    "\n",
    "  for train_sentence in re.split(sentence_endings, description):\n",
    "    # usamos el reconocimiento de aves anterior para encontrar las aves\n",
    "    # suponemos que la dbpedia tiene las aves bien identificadas\n",
    "    doc = nlp(train_sentence)\n",
    "    positions = []\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_ == tag and ent.ent_id_ != \"\":\n",
    "        vector = get_vector(ent)\n",
    "        positions.append((ent.start_char, ent.end_char, ent.ent_id_, vector))\n",
    "\n",
    "    if len(positions) > 0:\n",
    "      training_data.append(\n",
    "        (train_sentence, positions)\n",
    "      )\n",
    "\n",
    "def outer_join(lst1, lst2):\n",
    "  lst2_names = [name for name, annotations in lst2]\n",
    "  lst3 = [(name, annotations) for name, annotations in lst1 if not name in lst2_names]\n",
    "  return lst3\n",
    "\n",
    "test_data = random.sample(training_data, k=round(len(training_data)*0.2))\n",
    "\n",
    "# si quitamos las frases de test del modelo de entreno luego no vamos a poder detectar esas aves... no es ideal pero tampoco podemos asegurar que tengamos frases para todos los pájaros\n",
    "training_data = outer_join(training_data, test_data) \n",
    "\n",
    "for text, annotations in training_data[0:5]:\n",
    "  print(text, [(a, b, c) for a, b, c, d in annotations])\n",
    "  print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 187.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build train set\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = English()\n",
    "# no separar por \"-\"\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "# añadir entity_ruler del apartado anterior\n",
    "entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "nlp.add_pipe(\"entity_ruler\", source=entity_ruler)\n",
    "\n",
    "def build_db(nlp, data):\n",
    "  skips = 0\n",
    "  # the DocBin will store the example documents\n",
    "  db = DocBin()\n",
    "  for text, annotations in tqdm.tqdm(data):\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label, vector in annotations:\n",
    "      span = doc.char_span(start, end, label=label, vector=vector)\n",
    "      if span is None:\n",
    "        skips += 1\n",
    "      else:\n",
    "        ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    db.add(doc)\n",
    "  return (db, skips)\n",
    "\n",
    "(db, skips) = build_db(nlp, training_data)\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 222.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build test set\n",
    "(db, skips) = build_db(nlp, test_data)\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model https://spacy.io/usage/training\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# train the model\n",
    "!python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "!python -m spacy train config.cfg --output ../models/custom_ner --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy\n",
    "\n",
    "## powershell: (launch from console to use all cpu cores)\n",
    "# $env:KMP_DUPLICATE_LIB_OK = \"True\"\n",
    "# python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "# python -m spacy train config.cfg --output ../models/custom_ner --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    golden-fronteds fulvetta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">grey honeyeater</span>\n",
       "</mark>\n",
       " (Schoeniparus variegaticeps), also known as the gold-fronted fulvetta, is a species of bird in the family Pellorneidae</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    African goshawk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">african goshawk</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Accipiter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">accipiter</span>\n",
       "</mark>\n",
       " tachiro) is a species of African bird of prey in the genus \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Accipiter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">accipiter</span>\n",
       "</mark>\n",
       " which is the type genus of the family Accipitridae.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The type species is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hombron's kingfisher\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">hombron's kingfisher</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Actenoides\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">actenoides</span>\n",
       "</mark>\n",
       " hombroni)</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# juntamos el entreno previo con las patterns\n",
    "nlp = spacy.load(\"../models/custom_ner/model-best\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# entity_ruler = spacy.load(\"../models/entity_ruler\")\n",
    "# nlp.add_pipe(\"entity_ruler\", source=entity_ruler, before=\"ner\")\n",
    "\n",
    "# ner = spacy.load(\"en_core_web_sm\") # lg tiene vectores propios por lo que no podemos mezclar los vectores generados en nuestro modelo\n",
    "# nlp.add_pipe(\"ner\", name=\"base_ner\", source=ner, after=\"ner\")\n",
    "\n",
    "# comprobamos las pipes\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# try\n",
    "doc = nlp(\"The golden-fronteds fulvetta (Schoeniparus variegaticeps), also known as the gold-fronted fulvetta, is a species of bird in the family Pellorneidae\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "\n",
    "doc = nlp(\"The African goshawk (Accipiter tachiro) is a species of African bird of prey in the genus Accipiter which is the type genus of the family Accipitridae.\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "\n",
    "doc = nlp(\"The type species is Hombron's kingfisher (Actenoides hombroni)\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:17<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos encontrado 24 pájaros distintos de 33. 33 encontrados con ner y 0 encontrados con entity ruler. Ha habido 7 falsos casos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hemos encontrado 'yellow-bellied' con entrada en la dbpedia [Accipiter]'http://dbpedia.org/resource/Accipiter'.\\n\",\n",
       " \"Hemos encontrado 'purple-crowned lorikeet' con entrada en la dbpedia [Common kingfisher]'http://dbpedia.org/resource/Common_kingfisher'.\\n\",\n",
       " \"Hemos encontrado 'gull' con entrada en la dbpedia [Pycnonotus]'http://dbpedia.org/resource/Pycnonotus'.\\n\",\n",
       " \"Hemos encontrado 'common terns' con entrada en la dbpedia [Common kingfisher]'http://dbpedia.org/resource/Common_kingfisher'.\\n\",\n",
       " \"Hemos encontrado 'common in' con entrada en la dbpedia [Common kingfisher]'http://dbpedia.org/resource/Common_kingfisher'.\\n\"]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular para todas las review\n",
    "found_with_ruler = 0\n",
    "found_with_ner = 0\n",
    "error = 0\n",
    "maybe_matches = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  # if len(doc.ents) > 0:\n",
    "  #   displacy.render(doc, jupyter=True, style=\"ent\")\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == \"BIRD\":\n",
    "      (valid, alt_key, accuracy) = search_bird_dict(str(ent))\n",
    "      if valid:\n",
    "        found_with_ruler += 1\n",
    "        maybe_matches.append((ent.ent_id_, str(ent)))\n",
    "      else:\n",
    "        error += 1\n",
    "    else:\n",
    "      (valid, alt_key, accuracy) = search_bird_dict(str(ent))\n",
    "      if valid:\n",
    "        found_with_ner += 1\n",
    "        maybe_matches.append((ent.label_, str(ent)))\n",
    "      else:\n",
    "        error += 1\n",
    "\n",
    "# Pintar y guardar resultado\n",
    "result_lines = []\n",
    "for bird_key, original in set(maybe_matches):\n",
    "  bird = dict(birds[bird_key])\n",
    "  name = bird[\"name\"]\n",
    "  url = bird[\"url\"]\n",
    "  result_lines.append(f\"Hemos encontrado '{original}' con entrada en la dbpedia [{name}]'{url}'.\")\n",
    "\n",
    "\n",
    "result_lines_file = write(data_results_path, \"shorebirder_results_5\", result_lines)\n",
    "print(f\"Hemos encontrado {len(result_lines)} pájaros distintos de {len(maybe_matches)}.\",\n",
    "  f\"{found_with_ner} encontrados con ner y {found_with_ruler} encontrados con entity ruler.\",\n",
    "  f\"Ha habido {error} falsos casos\")\n",
    "open(result_lines_file, \"r\", encoding=\"utf-8\").readlines()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo entrenado es peor que en el apartado 4. (se ha añadido una validación porque daba muchos casos falsos)\n",
    "\n",
    "Se ha probado de unir la pipeline del apartado 4, `entity_ruler` (código comentado 2 bloques antes) y `ner` para mejorar los resultados y obtener contexto. Al no mejorarse los resultados del apartado 4 se ha abandonado el desarrollo dando el modelo del apartado 3 como el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El **intento 1** usando solamente los modelos pre-entrenados de spacy no son capaces de detectar aves. Previsible pero valía la pena intentar.\n",
    "\n",
    "El **intento 2** por planteamiento (y pista en el enunciado) es muy lento y se \"explota\" a la dbpedia a llamadas a base de datos para verificaciones simples. Tampoco podemos decir que sea una solución válida.\n",
    "\n",
    "El **intento 3** supone la primera solución válida al ejercicio, podría rebautizarse como *solución 1*. No solo descubre mayor número de pájaros sino que es \"preciso\". La pega que tiene su ejecución es la lentitud ya que requiere pasar pos el nlp de spacy para segmentar y encontrar entidades de segmento, y además, buscar si hay un nombre de pájaro realizando una comparación contra TODOS las posibles aves.\n",
    "\n",
    "El **intento 4** también solución válida mejora en eficiencia al intento 3. En este caso, sacrificamos precisión frente a rendimiento. El modelo es notablemente más rápido, pero no detecta variaciones. Una solución, podría ser alimentar a la pipe con distintas variaciones de los nombres de pájaros como añadir su variante al plural.\n",
    "\n",
    "El **intento 5**, posiblemente el más ambicioso y decepcionante a la vez, pretendía unir lo mejor de los intentos 3 y 4. Entrenar un modelo de clasificación. El modelo de clasificación consiste en entrenar una pipeline `ner` con los distintas frases donde \"manualmente\" le mostrásemos donde esta el nombre de cada pájaro y la etiqueta que le debe colocar. Además, para mejorar la detección se añadiría la pipeline del intento 4, y para obtener datos del contexto se añadiría una pipeline `ner` pre-entrenada. Las pruebas con 20-50 pájaros funcionaban increíblemente bien, tiempo de entreno largo, pero el modelo era bueno. Al entrenar con los 10369 pájaros el resultado fue muy distinto, el *entity_ruler* del intento 4 obtiene todos los match, y nuestro ner no consigue detectar nada. La guinda sobre el pastel es que nuestro ner \"obtiene\" pájaros que no existen.\n",
    "Supongo que la peor parte fue darse cuenta que tras 16h de \"training\" el modelo siguiera siendo menos efectivo que las dos soluciones anteriores estas siendo más \"simples\"."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
