{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Avistamiento de aves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos (scraping)\n",
    "\n",
    "Primero vamos a obtener los datos. Para ello nos visitaremos las web de avistamientos de aves [shorebirder](https://www.shorebirder.com/), [trevorsbirding](https://www.trevorsbirding.com/) y [dantallmansbirdblog](https://dantallmansbirdblog.blogspot.com/).\n",
    "\n",
    "Durante la visita a la web y haciendo uso del inspector (F12) podemos ver que las descripciones que necesitamos se encuentran en los tag de párrafo (entre *\\<p\\> TEXTO \\</p\\>*). Sabiendo eso vamos a crear funciones de utilidad que se encargarán de descargar el contenido de la web y extraer el texto.\n",
    "\n",
    "Las descargas las realizaremos en `data/raw` mientras que en `data/posts` guardaremos los textos encontrados.\n",
    "\n",
    "** `dantallmansbirdblog` tiene una estructura ligeramente diferente (entre *\\<p\\>\\</p\\> TEXTO \\<p\\>\\</p\\>*), a lo que tendremos que modificar la función `get_texts` (a continuación) para obtener sus textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O utils\n",
    "import os\n",
    "import re\n",
    "import wget\n",
    "import tqdm\n",
    "\n",
    "data_posts_cache = \"../data/cache\" # guardar resultados de queries a sparql\n",
    "data_raw_path = \"../data/raw\" # descargas\n",
    "data_posts_path = \"../data/posts\" # guardar los textos de los post scrapeados\n",
    "\n",
    "def maybe_mkdir(path):\n",
    "  try:\n",
    "    os.mkdir(path)\n",
    "  except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "maybe_mkdir(\"../data\")\n",
    "maybe_mkdir(data_raw_path)\n",
    "maybe_mkdir(data_posts_path)\n",
    "\n",
    "def download(url, out_label):\n",
    "  return wget.download(url, out=f\"{data_raw_path}/{out_label}\")\n",
    "\n",
    "def get_texts(filename):\n",
    "  file = open(filename, 'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "\n",
    "  # get texts\n",
    "  get_p = re.compile(r'<p>((.|\\n)*?)</p>')\n",
    "  texts = get_p.findall(text)\n",
    "\n",
    "  # remove styling and inner tags\n",
    "  remove_tags = re.compile(r'(<.*?>)|\\\\n| +(?= )|\\\\|\\&.+?\\;')\n",
    "  return map(lambda text: re.sub(remove_tags, \"\", str(text[0]).lower()), texts)\n",
    "\n",
    "def write(path, filename, data):\n",
    "  filepath = f\"{path}/{filename}.txt\"\n",
    "  file = open(filepath, \"a\", encoding=\"utf-8\")\n",
    "  for item in data:\n",
    "    file.write(str(item)+\"\\n\")\n",
    "  file.close()\n",
    "  return filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación haciendo uso de las funciones anteriores scrapeamos la home de `shorebirder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap shorebirder.com\n",
    "shorebirder_filename = \"shorebirder_home.html\"\n",
    "shorebirder_home = download(\"https://www.shorebirder.com/\", shorebirder_filename)\n",
    "posts = get_texts(shorebirder_home)\n",
    "shorebirder_posts_file = write(data_posts_path, shorebirder_filename, posts)\n",
    "\n",
    "open(shorebirder_posts_file, \"r\").readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para `trevorsbirding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap trevorsbirding.com\n",
    "trevorsbirding_filename = \"trevorsbirding_home.html\"\n",
    "trevorsbirding_home = download(\"https://www.trevorsbirding.com/\", trevorsbirding_filename)\n",
    "posts = get_texts(trevorsbirding_home)\n",
    "trevorsbirding_posts_file = write(data_posts_path, trevorsbirding_filename, posts)\n",
    "\n",
    "open(trevorsbirding_posts_file, \"r\").readlines()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 1: Usar spacy sin modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dependencias para el nlp\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[3:5]:\n",
    "  train = nlp(text)\n",
    "  displacy.render(train, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es capaz de identificar diferentes entidades dentro de las frases, pero no pájaros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 2: Usando sparql query para encontrar aves\n",
    "\n",
    "Idea: Usando el tokenizer de spacy como tokenizer trocear las frases. A partir de los token etiquetados como nombre (`NOUN`) lanzamos una petición a la dbpedia. Ya el resultado de la dbpedia nos dirá si existe y cual es su etiqueta / url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparQL class extension\n",
    "# Prefixes and Class based from https://github.com/ejrav/pydbpedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class SparqlEndpoint(object):\n",
    "\n",
    "    def __init__(self, endpoint, prefixes={}):\n",
    "        self.sparql = SPARQLWrapper(endpoint)\n",
    "        self.prefixes = {\n",
    "            \"dbo\": \"http://dbpedia.org/ontology/\",\n",
    "            \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "            \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "            \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "            \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "            \"dbpedia2\": \"http://dbpedia.org/property/\",\n",
    "            \"dbpedia\": \"http://dbpedia.org/\",\n",
    "            \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n",
    "            \"yago\": \"http://dbpedia.org/class/yago/\",\n",
    "            }\n",
    "        self.prefixes.update(prefixes)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "\n",
    "    def query(self, q):\n",
    "        lines = [\"PREFIX %s: <%s>\" % (k, r) for k, r in self.prefixes.items()]\n",
    "        lines.extend(q.split(\"\\n\"))\n",
    "        query = \"\\n\".join(lines)\n",
    "        self.sparql.setQuery(query)\n",
    "        results = self.sparql.query().convert()\n",
    "        return results[\"results\"][\"bindings\"]\n",
    "\n",
    "\n",
    "class DBpediaEndpoint(SparqlEndpoint):\n",
    "    def __init__(self, endpoint, prefixes = {}):\n",
    "        super(DBpediaEndpoint, self).__init__(endpoint, prefixes)\n",
    "\n",
    "s = DBpediaEndpoint(endpoint = \"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para buscar una ave dado su nombre\n",
    "def search_bird_dbpedia(token):\n",
    "  return s.query('''\n",
    "    SELECT *\n",
    "    WHERE {\n",
    "      ?bird a dbo:Bird ;\n",
    "            rdfs:label ?name ;\n",
    "            dbo:abstract ?comment .\n",
    "\n",
    "      filter (!isLiteral(?name) ||\n",
    "              langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "      filter (!isLiteral(?comment) ||\n",
    "              langmatches(lang(?comment), \"en\")) .\n",
    "\n",
    "      filter (CONTAINS(LCASE(STR(?name)), \"{token}\")) .\n",
    "    }\n",
    "    limit 5\n",
    "  '''.replace(\"{token}\", token))\n",
    "\n",
    "search_bird_dbpedia(\"falcon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "maybe_matches = {}\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in doc.noun_chunks:\n",
    "    for token in chunk:\n",
    "      if token.pos_ == 'NOUN':\n",
    "        results = search_bird_dbpedia(token.lemma_)\n",
    "        if len(results) > 0:\n",
    "          maybe_matches[token] = results\n",
    "          print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, funciona. Lo que es muy lento y estamos machacando la dbpedia a queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 3: Cachear / indexar la dbpedia\n",
    "\n",
    "Del intento anterior vamos a coger los resultados de todos los pájaros y lo convertiremos en un diccionario para que nos sea más fácil buscar y solo haremos n queries a la dbpedia. Por supuesto, esta estrategia es solo factible si el conjunto es finito. Como es nuestro caso, va haber n especies de pájaros, pero no va a estar creciendo dia a dia.\n",
    "\n",
    "### Estrategia\n",
    "- Obtener lista de todos los nombres de pájaros.\n",
    "- Con spacy analizaremos la entrada del avistamiento y obtenemos los `noun chunk`.\n",
    "  - Para hacer spacy más rápido vamos a deshabilitar las pipelines que no usemos, que son `lemmatizer` y `ner`.\n",
    "- Con cada `chunk` usando fuzzy-search en la lista de nombres de pájaros para encontrar aquellos chunk que parezcan nombres de pájaros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los pájaros. Obtenemos las descripciones para intentos posteriores.\n",
    "birds_sparql = s.query(\"\"\"\n",
    "  SELECT DISTINCT *\n",
    "  WHERE {\n",
    "    ?bird a dbo:Bird ;\n",
    "          rdfs:label ?name ;\n",
    "          dbo:abstract ?comment .\n",
    "\n",
    "    filter (!isLiteral(?name) ||\n",
    "            langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "    filter (!isLiteral(?comment) ||\n",
    "            langmatches(lang(?comment), \"en\")) .\n",
    "    \n",
    "  }\n",
    "  limit 10000\n",
    "\"\"\")\n",
    "\n",
    "birds_sparql += s.query(\"\"\"\n",
    "  SELECT DISTINCT *\n",
    "  WHERE {\n",
    "    ?bird a dbo:Bird ;\n",
    "          rdfs:label ?name ;\n",
    "          dbo:abstract ?comment .\n",
    "\n",
    "    filter (!isLiteral(?name) ||\n",
    "            langmatches(lang(?name), \"en\")) .\n",
    "\n",
    "    filter (!isLiteral(?comment) ||\n",
    "            langmatches(lang(?comment), \"en\")) .\n",
    "    \n",
    "  }\n",
    "  limit 10000\n",
    "  offset 10000\n",
    "\"\"\")\n",
    "\n",
    "write(\"../data\", \"birds\", birds_sparql)\n",
    "\n",
    "print(f\"Hemos obtenido los nombres de {len(birds_sparql)} pájaros\")\n",
    "for d in birds_sparql[0:5]:\n",
    "  print(d['name']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los datos en crudo a un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparql a diccionario\n",
    "birds = {}\n",
    "for bird in birds_sparql:\n",
    "  key = bird[\"name\"][\"value\"].lower()\n",
    "  birds[key] = {\n",
    "    \"name\": bird[\"name\"][\"value\"],\n",
    "    \"url\": bird[\"bird\"][\"value\"],\n",
    "    \"description\": bird[\"comment\"][\"value\"],\n",
    "  }\n",
    "bird_keys = birds.keys() # buscaremos por las key\n",
    "assert len(birds_sparql) == len(bird_keys) # aseguramos que no haya ninguna key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deshabilitar pipes de spacy que no necesitamos\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['lemmatizer', 'ner'])\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el `tokenizer` de spacy nos separa las palabras compuestas con guion.\n",
    "\n",
    "Para solucionarlo vamos a modificar el tokenizer para que no separe las palabras con guion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer que el tokenizer no separe palabras con guion\n",
    "# https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(\"Black-billed flycatcher\")\n",
    "displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genial! A demás conseguimos que `Black-billed` se detecte como adjetivo y no como adjetivo + verbo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busqueda de pájaros por fuzzysearch\n",
    "from fuzzywuzzy import fuzz\n",
    "def search_bird_dict(chunk):\n",
    "  best_match = (False, None, 0)\n",
    "  for key in bird_keys:\n",
    "    score = fuzz.ratio(key, chunk)\n",
    "    if score > 80 and score > best_match[-1]:\n",
    "      # print(f\"\\tchunk: '{chunk}' compare '{key}' score: '{score}'\")\n",
    "      best_match = (True, key, score)\n",
    "  return best_match\n",
    "\n",
    "def get_nouns(chunk):\n",
    "  only_nouns = []\n",
    "  for token in chunk:\n",
    "    if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
    "      only_nouns.append(token.lower_)\n",
    "\n",
    "  if len(only_nouns) > 0:\n",
    "    return \" \".join(only_nouns)\n",
    "  return None\n",
    "\n",
    "maybe_matches = []\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[0:5]:\n",
    "  doc = nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "        print(f\"Found '{bird_key}' in '{chunk}' with score of {score}\")\n",
    "          \n",
    "# Test result\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular para todas las review\n",
    "maybe_matches = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  for chunk in [get_nouns(chunk) for chunk in doc.noun_chunks]:\n",
    "    if chunk != None:\n",
    "      (found, bird_key, score) = search_bird_dict(chunk)\n",
    "      if found:\n",
    "        maybe_matches.append(bird_key)\n",
    "\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintar resultado\n",
    "result_lines = []\n",
    "for bird_key in set(maybe_matches):\n",
    "  bird = dict(birds[bird_key])\n",
    "  name = bird[\"name\"]\n",
    "  url = bird[\"url\"]\n",
    "  result_lines.append(f\"Hemos encontrado '{name}' con entrada en la dbpedia '{url}'.\")\n",
    "\n",
    "result_lines_file = write(data_posts_path, \"shorebirder_results_3\", result_lines)\n",
    "open(result_lines_file, \"r\").readlines()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera versión funcional sin machacar a la dbpedia. Pero sigue siendo muy lento. Vamos a seguir probando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento 4: Entrenar Spacy\n",
    "\n",
    "Hemos visto que podemos obtener los resultados usando una mezcla de código propio y spacy. En este apartado vamos a intentar usar solo spacy para encontrar los pájaros.\n",
    "\n",
    "La [pipeline](https://spacy.io/usage/spacy-101#pipelines) de spacy que se encarga de etiquetar las entities se llama [Entity Recognizer](https://spacy.io/api/entityrecognizer) o `ner`. Para poder etiquetar los pájaros vamos a usar las descripciones de la dbpedia para [entrenar spacy](https://spacy.io/usage/training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar las descripciones en datos de entreno y test para spacy\n",
    "import random\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# build training sentences\n",
    "training_data = [\n",
    "  # (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]), # example\n",
    "]\n",
    "\n",
    "tag = \"BIRD\"\n",
    "\n",
    "remove_parentesis_text = re.compile(r'\\(.*\\)')\n",
    "\n",
    "# mejorable pero hace su trabajo\n",
    "for key in tqdm.tqdm(bird_keys):\n",
    "  bird = birds[key]\n",
    "  bird_name = bird[\"name\"]\n",
    "\n",
    "  if len(bird_name) == 0:\n",
    "    continue\n",
    "  \n",
    "  bird_name = re.sub(remove_parentesis_text, \"\", str(bird_name))\n",
    "  train_sentence = bird[\"description\"]\n",
    "  positions = []\n",
    "  for match in re.finditer(strip_accents(bird_name), strip_accents(train_sentence), re.IGNORECASE):\n",
    "    positions.append(match.span() + (tag,))\n",
    "\n",
    "  training_data.append(\n",
    "    (train_sentence, positions)\n",
    "  )\n",
    "\n",
    "def outer_join(lst1, lst2):\n",
    "  lst3 = [value for value in lst1 if not value in lst2]\n",
    "  return lst3\n",
    "\n",
    "test_data = random.sample(training_data, k=round(len(training_data)*0.2))\n",
    "training_data = outer_join(training_data, test_data)\n",
    "\n",
    "for text, annotations in training_data[0:3]:\n",
    "  print(text, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "skips = 0\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for text, annotations in tqdm.tqdm(training_data):\n",
    "  doc = nlp(text)\n",
    "  ents = []\n",
    "  for start, end, label in annotations:\n",
    "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "    if span is None:\n",
    "      skips += 1\n",
    "    else:\n",
    "      ents.append(span)\n",
    "  filtered_ents = filter_spans(ents)\n",
    "  doc.ents = filtered_ents\n",
    "  db.add(doc)\n",
    "\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "skips = 0\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for text, annotations in test_data:\n",
    "  doc = nlp(text)\n",
    "  ents = []\n",
    "  for start, end, label in annotations:\n",
    "    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "    if span is None:\n",
    "      skips += 1\n",
    "    else:\n",
    "      ents.append(span)\n",
    "  filtered_ents = filter_spans(ents)\n",
    "  doc.ents = filtered_ents\n",
    "  db.add(doc)\n",
    "\n",
    "print(f\"Skipped {skips} entries\")\n",
    "db.to_disk(\"./birds_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model https://spacy.io/usage/training\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# train the model\n",
    "!python -m spacy init fill-config birds_config.cfg config.cfg\n",
    "!python -m spacy train config.cfg --output ./output --paths.train ./birds_train.spacy --paths.dev ./birds_test.spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"ner\"])\n",
    "nlp_entity = spacy.load(\"./output/model-best\")\n",
    "nlp.add_pipe(\"ner\", source=nlp_entity)\n",
    "\n",
    "for text in open(shorebirder_posts_file, \"r\").readlines()[4:5]:\n",
    "  doc = nlp(text)\n",
    "  if len(doc.ents) > 0:\n",
    "    displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que solo del contexto solo no es capaz de identificar las aves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solución! Nueva pipeline [entity_ruler](https://spacy.io/api/entityruler)\n",
    "\n",
    "En esta aproximación vamos a añadir una pipe más al nlp `en_core_web_lg` pre-entrenado de spacy. Para ello necesitamos hacer una lista de todos los patterns que queramos poner. Es decir, debemos introducir los nombres de los pájaros que queremos que se detecten como patterns y añadir la nueva pipe al nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando entity_ruler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "auxiliar_nlp = spacy.load(\"en_core_web_lg\", exclude=['tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "nlp = English()\n",
    "\n",
    "auxiliar_nlp.tokenizer = custom_tokenizer(auxiliar_nlp)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# Añadir los nombres de pájaros\n",
    "patterns = []\n",
    "for key in bird_keys:\n",
    "  bird = birds[key]\n",
    "  doc = nlp(bird[\"name\"])\n",
    "  pattern = []\n",
    "  for token in doc:\n",
    "    pattern.append({\n",
    "      \"LOWER\": token.lower_\n",
    "    })\n",
    "\n",
    "  patterns.append({\n",
    "    \"label\": tag,\n",
    "    \"pattern\": pattern\n",
    "  })\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"that feeding gull flock continued to produce by sucking in passers by. at one point a Bonaparte's gull got in on the action, and a flock of 21 common terns appeared from the east and eventually settled into that flock.\")\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular para todas las review\n",
    "maybe_matches = []\n",
    "for text in tqdm.tqdm(open(shorebirder_posts_file, \"r\").readlines()):\n",
    "  doc = nlp(text)\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == \"BIRD\":\n",
    "      maybe_matches.append(str(ent))\n",
    "\n",
    "print(maybe_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintar y guardar resultado\n",
    "result_lines = []\n",
    "for bird_key in set(maybe_matches):\n",
    "  bird = dict(birds[bird_key])\n",
    "  name = bird[\"name\"]\n",
    "  url = bird[\"url\"]\n",
    "  result_lines.append(f\"Hemos encontrado '{name}' con entrada en la dbpedia '{url}'.\")\n",
    "\n",
    "result_lines_file = write(data_posts_path, \"shorebirder_results_5\", result_lines)\n",
    "open(result_lines_file, \"r\").readlines()[0:5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
